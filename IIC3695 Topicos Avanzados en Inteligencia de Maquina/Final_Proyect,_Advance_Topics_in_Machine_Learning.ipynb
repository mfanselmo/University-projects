{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final Proyect, Advance Topics in Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ktLfXlTmDeUx"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oIe4lAIrCc5b"
      },
      "source": [
        "# Final delivery\n",
        "\n",
        "## Advance topics in Machine Learning\n",
        "\n",
        "### Predicting student's courses for next semester\n",
        "\n",
        "- Martin Anselmo\n",
        "- Guillermo Espinosa\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "For university professors and administrators, knowing how many students will take a course in advance allows them to plan and organize the subject in a better way, so that they can prepare the course better and create a better class environment.\n",
        "\n",
        "Due to the number of students that the engineering school of the Pontificia Universidad Cat√≥lica de Chile has, which exceeds 3000, knowing how many students are going to take a course allows you to better manage resources by opening or closing courses vacancies if necessary. A great cost (monetary and time) for the engineering school comes from how many teachers and students to assign for a each course and sections, and it is bad for both students and those in charge to have crowded or empty classrooms. For this reason, a system that tells how many vacancies are going to be needed per course based on different variables could be very useful.\n",
        "\n",
        "Due to the above, we propose a Bayesian Neural Network Model that predicts which courses is going to take each student based on what have they taken in previous semesters. We are going to use data obtained from the computer science department of course applications since 2015. It includes what each student has taken, in which semester and in which major and minor they are.\n",
        "\n",
        "All of this is was achived by using concepts learned from the course that can be applied to the real world.\n",
        "\n",
        "\n",
        "## Theoretical framework\n",
        "\n",
        "\n",
        "Our proposal is based on a Bayesian implementation of neural networks. These are models capable of classifying information based on different parameters, all through supervised learning, since we are passing many inputs with their respective classes, so that they \"learn\" how to classify.\n",
        "There are many types of neural networks, each with different uses:\n",
        "![tipos de redes](https://img.microsiervos.com/images2016/Neural-Networks-Chart.png)\n",
        "\n",
        "\n",
        "But we are going to look at the simplest, the multilayer perceptron, because it is the most used, and supported by libraries that do this type of work. It has an input layer, hidden layers and an output layer, with an activation function. In each connection there is a weight and an associated \"bias\", which are necessary for all calculations.\n",
        "\n",
        "#### Why networks Bayesian neural networks?\n",
        "\n",
        "In general, classical neural networks have given excellent results in various fields, being used in a multitude of contexts, such as image classification, text creation, etc. The problem they have is that they need an enormous amount of data to be trained, and in the event that it is necessary to predict something rare (such as a rare disease), there are not going to be many example data. This generally causes the network to over-train with the more common classes, and this is something that Bayesian networks come to fix.\n",
        "In our case, we want to predict courses of the computer department of the PUC, a course that is mandatory for engineering is introduction to programming (IIC1103), a typical problem of a classic neural network for this case would always be to predict this course. The use of Bayesian networks will prevent this.\n",
        "\n",
        "#### Bayesian neural networks\n",
        "\n",
        "The Bayesian part comes in two, each with an associated problem:\n",
        "1. Work with prior knowledge (* prior *), corresponding to different probability distributions on the weights of the network. The problem is, that in neural networks this knowledge has little to do with the structure itself. To solve this, we are going to work with * priors * that are not very informative for us, but which will also influence the network's decision making a lot. This is because it will no longer be \"yes\" or \"no\", but \"maybe\", and it will depend a lot on the current state of the network.\n",
        "2. To obtain correct predictions, the posterior one must be integrated. This is very demanding, or impossible in some cases. To solve this, we are going to work with samples on the network. As we are going to work with distributions on the parameters, after training we will have a network with the hyperparameters of each of the distributions, so we can take samples of networks, and thus decide.\n",
        "\n",
        "Then, the structure of the Bayesian network is defined in a very similar way to a multilayer perceptron, only now instead of fixed parameters, we are going to have probability distributions on the weights.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Zhenhua_Li12/publication/328757994/figure/fig1/AS:689808791830528@1541474640467/A-Bayesian-neural-network-with-one-hidden-layer.ppm)\n",
        "\n",
        "#### Training\n",
        "\n",
        "There are several ways to train ranking models. The best known is backpropagation, which is based on taking gradients, and changing parameters in the direction of maximum growth. This method is not so good for Bayesian networks, since they take into account uncertainties, and backpropagation can go in that direction. We are going to focus on a method called * Bayes by Backprop *, created specifically to train networks of this type. It is based on that instead of training a single model, it trains an assembly of models, taking weights from a joint distribution. This method only doubles the number of parameters, but trains an infinite assembly of models using Monte Carlo methods (actually, of variational inference, since the methods already named are still very difficult in this context).\n",
        "\n",
        "It is impossible to calculate the integral of the parameters, so a variational inference method will be used to estimate the distribution, using ELBO as a loss function. In each iteration / epoch our program will take several samples of the parameters, and be guided by them.\n",
        "It should be noted that this is going to be done by the Pyro library, which we will talk about later.\n",
        "\n",
        "#### Choice of priors\n",
        "\n",
        "Earlier we said that there is a problem in this part, although we have previous knowledge of the problem (that certain students should take as mesh), it is difficult to reflect this in the structure of a neural network. This is why we decided on normal distributions with initial parameters 0 and 1, for the mean and standard deviation respectively.\n",
        "\n",
        "#### Obtaining predictions\n",
        "\n",
        "To obtain model predictions, different samples of models will be generated. For each one of these models, they will be asked to which class the given input corresponds and if there is consensus on the class to which the input belongs, it will be decided by this. The Bayesian part comes when there is none consensus. Then, the network is going to be allowed to say \"I don't know\", which is a very valuable tool in our case. For example, when asked by a student who is not necessarily going to take a course of computing in the following semester, the network will not be forced to choose a class (course), but will say that it is not certain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H1XiD6eoM120"
      },
      "source": [
        "## Methodology\n",
        "\n",
        "In order to predict with Bayesian networks which courses students will be taking next semester, we had to carry out several steps, which are detailed below.\n",
        "\n",
        "### 1. Investigating Bayesian Neural Networks\n",
        "Not much information is available on Bayesian methods, especially Bayesian neural networks. However, we found some python libraries that do this, such as Edward or Pyro, along with their respective tutorials. We decided to use Pyro, since it seemed easier to use, and was well documented.\n",
        "\n",
        "### 2. Data processing\n",
        "Pyro is an implementation of Bayesian methods based on the Pytorch machine learning library, which is why we first had to bring the data we had to a training set, which would work with the aforementioned libraries.\n",
        "\n",
        "We start by defining the structure of the network, a simple multilayer perceptron, with a hidden layer. For this, we decided to make an input array that contains all the information necessary to classify correctly, and an output array that contains the the courses that a certain input (student) will take.\n",
        "We obtained the data from the Computer Department, and consisted of an excel file with the following information in each row:\n",
        "\n",
        "- student id (anonymous)\n",
        "- Semester in which student is took the course\n",
        "- Career\n",
        "- Program which student is enrolled\n",
        "- Major and minor\n",
        "- Semester enrolled\n",
        "- Course\n",
        "- More information which is unnecesary\n",
        "\n",
        "We had to process all this information (detailed later) to obtain an input that can be entered into the neural network.\n",
        "\n",
        "### 3. Classic Neural Network\n",
        "\n",
        "We decided to train a classic neural network, to compare results. It has the same structure as the Bayesian network, only with \"fixed\" parameters instead of distributions.\n",
        "\n",
        "### 4. Bayesian neural network\n",
        "\n",
        "Using the same network structure as in the previous part, we build and train the network.\n",
        "\n",
        "### 5. Changes compared to the last version\n",
        "\n",
        "We decided to make several changes to the structures of our delivery 1 networks, with the main objective of making them better represent reality.\n",
        "\n",
        "1. Network structure\n",
        "    - We tried different network structures, with more hidden layers, different activation functions.\n",
        "    - Network output: before, we had a log-softmax activation function, which was wrong, because is a function that expectes one class and only to be the correct one, and this is not the case.\n",
        "    - Since you can take more than one courses, we had to change the loss function for a more appropriate one. We chose `MultiLabelSoftMarginLoss`, which does exactly this.\n",
        "    - Now we have to choose if a student takes 0 or more courses, this is why we add confidence to the choice.\n",
        "2. Bayesian part\n",
        "    - Since the structure of the classical network is shared with the Bayesian one, for the Bayesian part of our project we made the same changes as in (1)\n",
        "    - Since the Bayesian network makes a decision with several samples of networks, we can calculate the variance in the output. By doing this, we can see if the result comes by chance or if the network really knows what it is saying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XRoGka1eky2D"
      },
      "source": [
        "### Code\n",
        "\n",
        "We start by importing all the libraries that we will use, the main ones will be pytorch, which implements an interface to implement neural networks easily, and pyro, which allows using Bayesian methods on pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dS0eOA2mwS52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c40aab0e-0716-4d67-d342-98ce24dd5f5e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0SEH-SamracD",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izCvafSoZb6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a7e2724b-c400-4d8d-fb64-b6a6f9369a02"
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.5.1+cu101)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->pyro-ppl) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XqlJbwhcrjEF",
        "colab": {}
      },
      "source": [
        "import pyro\n",
        "from pyro.distributions import Normal, Multinomial\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam, Adadelta"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hJRdSGMgurJJ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gyyBxOiYMGp4"
      },
      "source": [
        "We read the data, (make sure they are in the same directory), and clean the columns that we don't need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PrzXP4teQiK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ba55124b-64a8-436d-9eb9-a173f2317c38"
      },
      "source": [
        "\n",
        "try:\n",
        "    students_enrollment = pd.read_excel(\"Analisis_vacantes_IIC.xlsx\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    students_enrollment = pd.read_excel(\"/content/drive/My Drive/proyectIDM/Analisis_vacantes_IIC.xlsx\")\n",
        "\n",
        "students_enrollment = students_enrollment[students_enrollment['Nivel Programa'] == 'PR - Pregrado']\n",
        "students_enrollment = students_enrollment[students_enrollment['Escuela Alumno'] == '04 - Ingenier√≠a']\n",
        "students_enrollment = students_enrollment.drop(columns=['Escuela Curso',\n",
        "                                          'Nivel Principal ',\n",
        "                                          'NRC','Materia', \n",
        "                                          'N√∫mero Curso', \n",
        "                                          'Secci√≥n', \n",
        "                                          'CURSO-SEC', \n",
        "                                          'M√©todo verificaci√≥n Requisitos', \n",
        "                                          'Programa', \n",
        "                                          'Status Alumno', \n",
        "                                          'Vacante Asignada (S/N)', \n",
        "                                          'Sobrepaso (S/N)', \n",
        "                                          'Status Inscripci√≥n Curso ', \n",
        "                                          'Escuela Alumno'])\n",
        "students_enrollment.rename(columns = \n",
        "                    {'periodo': 'semestre',\n",
        "                     'CURSO': 'curso',\n",
        "                     'Nombre Curso':'nombre',\n",
        "                     'Escuela Alumno': 'escuela',\n",
        "                     'Nivel Programa': 'programa',\n",
        "                     'Periodo Admisi√≥n': 'admision',\n",
        "                     'MAJOR BANNER': 'major',\n",
        "                     'MINOR BANNER':'minor',\n",
        "                     'N¬∞ ALUMNO AN√ìNIMO': 'alumno'}, inplace = True)\n",
        "\n",
        "students_enrollment = students_enrollment.replace(np.nan, 'NA', regex=True)\n",
        "\n",
        "order_semesters = {\n",
        "    200420: 0, \n",
        "    200422: 1, \n",
        "    200520: 2, \n",
        "    200522: 3, \n",
        "    200620: 4, \n",
        "    200622: 5, \n",
        "    200720: 6, \n",
        "    200722: 7, \n",
        "    200820: 8, \n",
        "    200822: 9, \n",
        "    200920: 10, \n",
        "    200922: 11, \n",
        "    201020: 12, \n",
        "    201022: 13, \n",
        "    201120: 14, \n",
        "    201122: 15,\n",
        "    201220: 16, \n",
        "    201222: 17, \n",
        "    201320: 18, \n",
        "    201322: 19,\n",
        "    201420: 20, \n",
        "    201422: 21, \n",
        "    201520: 22, \n",
        "    201522: 23,\n",
        "    201620: 24, \n",
        "    201622: 25, \n",
        "    201720: 26, \n",
        "    201722: 27,\n",
        "    201820: 28, \n",
        "    201822: 29, \n",
        "    201920: 30, \n",
        "    201922: 31,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# students_enrollment['semestre'], mapping_semester = pd.Series(students_enrollment['semestre']).factorize()\n",
        "students_enrollment['curso'], mapping_course = pd.Series(students_enrollment['curso']).factorize()\n",
        "students_enrollment['nombre'], mapping_name = pd.Series(students_enrollment['nombre']).factorize()\n",
        "# students_enrollment['admision'], mapping_admision = pd.Series(students_enrollment['admision']).factorize()\n",
        "students_enrollment['major'], mapping_major = pd.Series(students_enrollment['major']).factorize()\n",
        "students_enrollment['minor'], mapping_minor = pd.Series(students_enrollment['minor']).factorize()\n",
        "students_enrollment['programa'], mapping_program = pd.Series(students_enrollment['programa']).factorize()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "students_enrollment['semestre_actual'] = students_enrollment.apply (lambda row: order_semesters[row['semestre']] - order_semesters[row['admision']], axis=1)\n",
        "\n",
        "\n",
        "# Reordenar columnas y sacar la de nombres \n",
        "cols = students_enrollment.columns.tolist()\n",
        "\n",
        "# print(cols)\n",
        "\n",
        "cols = [cols[1]] + [cols[3]] + [cols[4]] + cols[6:]\n",
        "\n",
        "students_enrollment = students_enrollment[cols]\n",
        "\n",
        "students_enrollment.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>curso</th>\n",
              "      <th>alumno</th>\n",
              "      <th>programa</th>\n",
              "      <th>major</th>\n",
              "      <th>minor</th>\n",
              "      <th>semestre_actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    curso  alumno  programa  major  minor  semestre_actual\n",
              "18      0      11         0      0      0                6\n",
              "19      1      11         0      0      0                6\n",
              "20      2      11         0      0      0                6\n",
              "21      1      11         0      0      0                7\n",
              "22      0      11         0      0      0                8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0a7t3Bf6Mrz"
      },
      "source": [
        "Then, we transform the inputs into a form that the classifier can understand, in this case, the input vector\n",
        "It will be a list of ones and zeros that represent which courses a student has taken in each previous semester, in which semester this is and to which major and minor program it belongs.\n",
        "\n",
        "### Input\n",
        "We define a total of 31 semesters (most will be empty), and 40 courses of the DCC. The input corresponds to:\n",
        "\n",
        "#### Part 1\n",
        "Semester numbor 0 of student:\n",
        "$$\n",
        "[\\text{onehot encoding of courses the student have taken up to that semester}]\n",
        "== [0, \\ldots, 0]\n",
        "$$\n",
        "Since he/she has not taken any, they are only zeros\n",
        "\n",
        "Semester 1 of student: (assuming on semester 0 he took only *introduccion a la programacion* and it's index is 0)\n",
        "$$\n",
        "[\\text{onehot encoding of courses the student have taken up to that semester}]\n",
        "== [1, \\ldots, 0]\n",
        "$$ \n",
        "\n",
        "So on until semester 31.\n",
        "\n",
        "#### Part 2\n",
        "One hot encoding of the semester where you are consulting what courses you want to take.\n",
        "\n",
        "#### Parte 3\n",
        "One hot encoding of the major in which the student is enrolled.\n",
        "\n",
        "#### Parte 4\n",
        "One hot encoding of the minor in which the student is enrolled.\n",
        "\n",
        "#### Parte 5\n",
        "One hot encoding of the program that the student is enrolled.\n",
        "\n",
        "#### Total\n",
        "\n",
        "$$\\text{sizee of input} = n_{semesters}*n_{courses} + n_{courses} + n_{majors} + n_{minors} + n_{programs}$$\n",
        "\n",
        "\n",
        "### Output\n",
        "\n",
        "Vector $n_{courses}$ long with ones indicating which courses the sudent is going to take next semester (part 2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FY71QZS_UPFr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d1f0bfc-b493-4914-d545-e95fd665cbcb"
      },
      "source": [
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "n_semesters = len(order_semesters.values())\n",
        "n_courses = len(mapping_name)\n",
        "n_majors = len(mapping_major)\n",
        "n_minors = len(mapping_minor)\n",
        "n_programs = len(mapping_program)\n",
        "\n",
        "for student in students_enrollment.groupby([\"alumno\"]):\n",
        "    a = student[1].sort_values('semestre_actual')\n",
        "    in_ = [0 for i in range(n_semesters*n_courses)] # Previous semesters\n",
        "    in_ += [0 for i in range(n_semesters)] # Actual semester\n",
        "    in_ += [0 for i in range(n_majors)]\n",
        "    in_ += [0 for i in range(n_minors)]\n",
        "    in_ += [0 for i in range(n_programs)]\n",
        "  \n",
        "    for semester in a.groupby(\"semestre_actual\"):\n",
        "        semester_actual = semester[0]\n",
        "        # print(semester[1])\n",
        "        taken = []\n",
        "        for tupla in semester[1].values: \n",
        "      \n",
        "            course = tupla[0]\n",
        "            taken.append(course)\n",
        "            program = tupla[2]\n",
        "            major = tupla[3]\n",
        "            minor = tupla[4]\n",
        "            \n",
        "\n",
        "            # Pos semester actual = n_semesters*n_courses + semester_actual\n",
        "            in_[n_semesters*n_courses + semester_actual] = 1\n",
        "\n",
        "            # Major = n_semesters*n_courses + n_semesters + major\n",
        "            in_[n_semesters*n_courses + n_semesters + major] = 1\n",
        "\n",
        "            # Minor = n_semesters*n_courses + n_semesters + n_majors + minor\n",
        "            in_[n_semesters*n_courses + n_semesters + n_majors + minor] = 1\n",
        "\n",
        "            # program = n_semesters*n_courses + n_semesters + n_majors + n_minors + program\n",
        "            in_[n_semesters*n_courses + n_semesters + n_majors + n_minors + program] = 1\n",
        "\n",
        "      \n",
        "        inputs.append(in_[:])\n",
        "        in_[n_semesters*n_courses + semester_actual] = 0\n",
        "        \n",
        "        \n",
        "        out = [0 for i in range(n_courses)]\n",
        "\n",
        "        # I update the following semesters (The courses each student have taken so far)\n",
        "        for i in taken:\n",
        "          # Position of the course is semester_actual*40 + pos_course\n",
        "            pos_course = semester_actual*n_courses + i\n",
        "            in_[pos_course] = 1\n",
        "            out[i] = 1\n",
        "        outputs.append(out)\n",
        "        \n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "\n",
        "inputs[0]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VInHiRosMQcZ"
      },
      "source": [
        "In addition, each of the categories has been passed to numerical data, for easier handling, and we have saved the corresponding mappings.\n",
        "\n",
        "We separate training data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-CwXO_R4gPas",
        "colab": {}
      },
      "source": [
        "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=0.33, random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "084IoWm6lhxy"
      },
      "source": [
        "Pasamos el input a lo que necesita torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "on_txh8yztbk",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Torch necesita lista de tuplas (tensor, clase)\n",
        "trainset = []\n",
        "testset = []\n",
        "\n",
        "for input_, clase in zip(inputs_train, outputs_train):\n",
        "  \n",
        "    classes = np.where(clase == 1)[0]\n",
        "    n = classes.shape[0]\n",
        "\n",
        "    for i in range(n):\n",
        "        trainset.append((torch.tensor(input_, dtype=torch.float), classes[i]))\n",
        "\n",
        "\n",
        "  \n",
        "for input_, clase in zip(inputs_test, outputs_test):\n",
        "  \n",
        "    classes = np.where(clase == 1)[0]\n",
        "    n = classes.shape[0]\n",
        "\n",
        "    for i in range(n):\n",
        "        testset.append((torch.tensor(input_, dtype=torch.float), classes[i]))\n",
        "        \n",
        "        \n",
        "\n",
        "inputs_train_2 = np.array_split(inputs_train, 50) \n",
        "inputs_test_2 = np.array_split(inputs_test, 25)\n",
        "outputs_train_2 = np.array_split(outputs_train, 50)\n",
        "outputs_test_2 = np.array_split(outputs_test, 25)\n",
        "for i in range(len(inputs_train_2)):\n",
        "    inputs_train_2[i] = torch.from_numpy(inputs_train_2[i]).to(device).to(torch.float)\n",
        "    outputs_train_2[i] = torch.from_numpy(outputs_train_2[i]).to(device).to(torch.float)\n",
        "    \n",
        "for i in range(len(inputs_test_2)): \n",
        "    inputs_test_2[i] = torch.from_numpy(inputs_test_2[i]).to(device).to(torch.float)\n",
        "    outputs_test_2[i] = torch.from_numpy(outputs_test_2[i]).to(device).to(torch.float)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OypGsIVc1kGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d1b2c364-f766-4f5e-c7b3-94d069e28a5c"
      },
      "source": [
        "inputs_test_2[i]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WUKQuwJ1z1Hu",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 64\n",
        "test_batch_size = 256\n",
        "epochs = 100\n",
        "lr = 0.01\n",
        "momentum = 0.5\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=2)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
        "#                                           shuffle=False, num_workers=2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W2iBZXZqL8uv"
      },
      "source": [
        "## Neural network\n",
        "\n",
        "In the neural network model (both basic and Bayesian), a class is created that defines the structure that it will have. For this first installment, it was decided to use a simple one-layer neural network, which in the future will try to structure in a better way for better performance.\n",
        "\n",
        "In this project we are going to compare the classic implementation of a multilayer perceptron, and a Bayesian neural network based on the same structure, so we will be able to see which one gives better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZSJ6WwEz_i9",
        "colab": {}
      },
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1 ,output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.out = nn.Linear(hidden_size_1, output_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.out(output)\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIhEJSVt95U6"
      },
      "source": [
        "In order for neural networks to be trained, a train function is created that controls how the network is trained. That is, see what loss function it uses, how big the jump is in each step, how much data is used in each iteration and what optimizer is used.\n",
        "\n",
        "In addition, it is necessary to be able to test the data, therefore the test function is created that evaluates how the data was created with the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tHLVos_5AZOX"
      },
      "source": [
        "In summary, the basic neural network (not yet Bayesian) that was created is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kGExR6t0DBW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "76a29897-35e0-4365-d7af-1cea468d6c06"
      },
      "source": [
        "print(\"Input in: \", n_semesters*n_courses + n_semesters + n_majors + n_minors + n_programs)\n",
        "net = NN(n_semesters*n_courses + n_semesters + n_majors + n_minors + n_programs, 3000, n_courses).to(device)\n",
        "\n",
        "summary(net, trainset[0][0].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input in:  1443\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 3000]       4,332,000\n",
            "            Linear-2                   [-1, 40]         120,040\n",
            "================================================================\n",
            "Total params: 4,452,040\n",
            "Trainable params: 4,452,040\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 16.98\n",
            "Estimated Total Size (MB): 17.01\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IFRd3NK9AWhQ"
      },
      "source": [
        "Here we proceed to train the neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76zW_TPX0HJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "daeccea7-574c-47a2-db63-535a426d9457"
      },
      "source": [
        "optimizer = optim.Adadelta(net.parameters())\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "\n",
        "losses_total = []\n",
        "accuracy_total = []\n",
        "\n",
        "epochs = 400\n",
        "for epoch in range(epochs):\n",
        "    losses = []\n",
        "    for sample, label  in zip(inputs_train_2, outputs_train_2):\n",
        "        output = net(sample)\n",
        "        loss = criterion(output, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.mean().item())\n",
        "        \n",
        "    losses_total.append(np.mean(losses))\n",
        "    if epoch%50 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"epoch\", epoch, 'Loss: {:.3f}'.format(np.mean(losses)))\n",
        "        result = []\n",
        "        size = []\n",
        "        for data0, data1 in zip(inputs_test_2, outputs_test_2):\n",
        "            output = (Multinomial(logits= net(data0)).mean > 0.3).to(torch.float)\n",
        "            compa = torch.eq(output, data1)\n",
        "            result.append(torch.sum(torch.sum(compa, dim = 1) == 40).item())\n",
        "            size.append(len(output))\n",
        "        print('Accuracy: ', sum(result)/sum(size))\n",
        "        accuracy_total.append(sum(result)/sum(size))\n",
        "    else:\n",
        "        accuracy_total.append(sum(result)/sum(size))\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 Loss: 0.334\n",
            "Accuracy:  0.0\n",
            "epoch 50 Loss: 0.082\n",
            "Accuracy:  0.40155296727676093\n",
            "epoch 100 Loss: 0.074\n",
            "Accuracy:  0.4317803660565724\n",
            "epoch 150 Loss: 0.070\n",
            "Accuracy:  0.4425956738768719\n",
            "epoch 200 Loss: 0.067\n",
            "Accuracy:  0.4509151414309484\n",
            "epoch 250 Loss: 0.064\n",
            "Accuracy:  0.45174708818635606\n",
            "epoch 300 Loss: 0.062\n",
            "Accuracy:  0.45368829728230725\n",
            "epoch 350 Loss: 0.060\n",
            "Accuracy:  0.45701608430393786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kPDpWvwomuWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "a6e97251-69f8-4e61-9a0f-e45f6492dc94"
      },
      "source": [
        "plt.plot(range(400), losses_total)\n",
        "plt.title(\"Epoch vs Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()\n",
        "plt.plot(range(400), accuracy_total)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.title(\"Epoch vs Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdEmcrs_C06g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "96a99a5f-d8f4-4b79-cdc2-70a48e0b9ef5"
      },
      "source": [
        "result = []\n",
        "size = []\n",
        "for data0, data1 in zip(inputs_test_2, outputs_test_2):\n",
        "    output = (Multinomial(logits= net(data0)).mean > 0.5).to(torch.float)\n",
        "    compa = torch.eq(output, data1)\n",
        "    result.append(torch.sum(torch.sum(compa, dim = 1) == 40).item())\n",
        "    size.append(len(output))\n",
        "print(\"How many did it get in each batch:\", result)\n",
        "print(\"size of batch:\", size)\n",
        "print(\"Accuracy: \", sum(result)/sum(size))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How many did it get in each batch: [65, 61, 62, 69, 58, 61, 59, 56, 58, 57, 63, 59, 61, 57, 63, 65, 64, 65, 58, 66, 61, 63, 69, 54, 63]\n",
            "size of batch: [145, 145, 145, 145, 145, 145, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144]\n",
            "Accuracy:  0.4262340543538547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HzsTFcncmRh6"
      },
      "source": [
        "As you can see, we immediately reached a decent result (considering that we are saying it is correct only if it gets all the courses right)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3dlb5XGrTNb"
      },
      "source": [
        "## Bayesian Neural Network\n",
        "\n",
        "For the Bayesian neural network, the same network structure is used as in the basic network, we even use the same class as in the previous network. The difference lies in the model that is established for the links. In the first box, a network is created as in the previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GIEcs9dRrXnL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "92274897-9cf7-4ec2-bfa3-720d73a55d00"
      },
      "source": [
        "net_bayesian = NN(n_semesters*n_courses + n_semesters + n_majors + n_minors + n_programs, 1000, n_courses).to(device)\n",
        "summary(net_bayesian, trainset[0][0].shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 1000]       1,444,000\n",
            "            Linear-2                   [-1, 40]          40,040\n",
            "================================================================\n",
            "Total params: 1,484,040\n",
            "Trainable params: 1,484,040\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 5.66\n",
            "Estimated Total Size (MB): 5.67\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "boRDXQVsBzDq"
      },
      "source": [
        "In this function and the next one is where the important part of the Bayesian model resides. The following function defines a model for the links in the network, giving them a prior. Since we have no default values, a non-informative prior is established (a normal with zero mean and variance 1). Then, we use the pyro library to create a model that we adjust for our weights and compare the results of training the model with that of the actual data (to train). Furthermore, since we do not know the distribution of weights, variational bayes are used to approximate the distribution of weights to normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IwrQBpOaufJG",
        "colab": {}
      },
      "source": [
        "def model(x_data, y_data):\n",
        "    \n",
        "    fc1w_prior = Normal(loc=torch.zeros_like(net_bayesian.fc1.weight), scale=torch.ones_like(net_bayesian.fc1.weight))\n",
        "    fc1b_prior = Normal(loc=torch.zeros_like(net_bayesian.fc1.bias), scale=torch.ones_like(net_bayesian.fc1.bias))\n",
        "\n",
        "#     fc2w_prior = Normal(loc=torch.zeros_like(net_bayesian.fc2.weight), scale=torch.ones_like(net_bayesian.fc2.weight))\n",
        "#     fc2b_prior = Normal(loc=torch.zeros_like(net_bayesian.fc2.bias), scale=torch.ones_like(net_bayesian.fc2.bias))\n",
        "    \n",
        "    outw_prior = Normal(loc=torch.zeros_like(net_bayesian.out.weight), scale=torch.ones_like(net_bayesian.out.weight))\n",
        "    outb_prior = Normal(loc=torch.zeros_like(net_bayesian.out.bias), scale=torch.ones_like(net_bayesian.out.bias))\n",
        "    \n",
        "#     priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    # lift module parameters to random variables sampled from the priors\n",
        "    lifted_module = pyro.random_module(\"module\", net_bayesian, priors)\n",
        "    # sample a regressor (which also samples w and b)\n",
        "    lifted_reg_model = lifted_module()\n",
        "    lhat = lifted_reg_model(x_data)\n",
        "    pyro.sample(\"obs\", Multinomial(logits=lhat), obs = y_data)\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAqViePxQ3AE"
      },
      "source": [
        "Esta funci√≥n gu√≠a se encarga de que hacer cuando se cree un modelo. Es decir, samplea un modelo de red neuronal con los modelos estimados de pesos para la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91dDt9O_un76",
        "colab": {}
      },
      "source": [
        "softplus = torch.nn.Softplus()\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "    \n",
        "    # First layer weight distribution priors\n",
        "    fc1w_mu = torch.randn_like(net_bayesian.fc1.weight)\n",
        "    fc1w_sigma = torch.randn_like(net_bayesian.fc1.weight)\n",
        "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
        "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
        "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
        "    # First layer bias distribution priors\n",
        "    fc1b_mu = torch.randn_like(net_bayesian.fc1.bias)\n",
        "    fc1b_sigma = torch.randn_like(net_bayesian.fc1.bias)\n",
        "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
        "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
        "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
        "#     # Second layer weight distribution priors\n",
        "#     fc2w_mu = torch.randn_like(net_bayesian.fc2.weight)\n",
        "#     fc2w_sigma = torch.randn_like(net_bayesian.fc2.weight)\n",
        "#     fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
        "#     fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
        "#     fc2w_prior = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
        "#     # Second layer bias distribution priors\n",
        "#     fc2b_mu = torch.randn_like(net_bayesian.fc2.bias)\n",
        "#     fc2b_sigma = torch.randn_like(net_bayesian.fc2.bias)\n",
        "#     fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
        "#     fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
        "#     fc2b_prior = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
        "    # Output layer weight distribution priors\n",
        "    outw_mu = torch.randn_like(net_bayesian.out.weight)\n",
        "    outw_sigma = torch.randn_like(net_bayesian.out.weight)\n",
        "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
        "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
        "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
        "    # Output layer bias distribution priors\n",
        "    outb_mu = torch.randn_like(net_bayesian.out.bias)\n",
        "    outb_sigma = torch.randn_like(net_bayesian.out.bias)\n",
        "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
        "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
        "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
        "#     priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    \n",
        "    lifted_module = pyro.random_module(\"module\", net_bayesian, priors)\n",
        "    \n",
        "    return lifted_module()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LinMNkJNRtL2"
      },
      "source": [
        "Here we specify the optimizer along with setting the model. For this case the optimizer Adam was chosen, but for the next installment you can consider using a different optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9W4cuUDGE1i7",
        "colab": {}
      },
      "source": [
        "# Podemos cambiar el optimizador\n",
        "# optim = Adam({\"lr\": 0.01})\n",
        "optim = Adadelta({\"lr\": 0.01})\n",
        "\n",
        "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JaXir7lzR3qp"
      },
      "source": [
        "Finally, we train the network. A number of iterations equal to 1000 were used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uDq_ATCDuOvv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ae5f629-f8bf-4e81-e8bd-9f1d98e49b43"
      },
      "source": [
        "epochs_bayesiano = 1000\n",
        "\n",
        "losses_total_bayesiano = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs_bayesiano):\n",
        "    loss = 0\n",
        "    \n",
        "    for data0, data1 in zip(inputs_train_2, outputs_train_2):\n",
        "        # calculate the loss and take a gradient step\n",
        "        loss += svi.step(data0, data1)\n",
        "    torch.cuda.empty_cache()\n",
        "    normalizer_train = len(inputs_train)\n",
        "    total_epoch_loss_train = loss / normalizer_train\n",
        "    losses_total_bayesiano.append(total_epoch_loss_train)\n",
        "    print(\"Epoch \", epoch, \" Loss \", total_epoch_loss_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  Loss  9552.021430450639\n",
            "Epoch  1  Loss  9531.483755657657\n",
            "Epoch  2  Loss  9496.375575563234\n",
            "Epoch  3  Loss  9485.011421711788\n",
            "Epoch  4  Loss  9477.374662402419\n",
            "Epoch  5  Loss  9454.86917373922\n",
            "Epoch  6  Loss  9430.367771919544\n",
            "Epoch  7  Loss  9414.49724804956\n",
            "Epoch  8  Loss  9392.28048649139\n",
            "Epoch  9  Loss  9370.527537523227\n",
            "Epoch  10  Loss  9343.365457938984\n",
            "Epoch  11  Loss  9329.207279373672\n",
            "Epoch  12  Loss  9290.887191630172\n",
            "Epoch  13  Loss  9279.426041291017\n",
            "Epoch  14  Loss  9263.234085488504\n",
            "Epoch  15  Loss  9226.773647459238\n",
            "Epoch  16  Loss  9200.623469182157\n",
            "Epoch  17  Loss  9180.152356739203\n",
            "Epoch  18  Loss  9161.244977863747\n",
            "Epoch  19  Loss  9145.000394230641\n",
            "Epoch  20  Loss  9114.152307032171\n",
            "Epoch  21  Loss  9088.972018184419\n",
            "Epoch  22  Loss  9074.164874721964\n",
            "Epoch  23  Loss  9037.180736625662\n",
            "Epoch  24  Loss  9027.33118182288\n",
            "Epoch  25  Loss  8995.569658343267\n",
            "Epoch  26  Loss  8980.536047049047\n",
            "Epoch  27  Loss  8958.34285723497\n",
            "Epoch  28  Loss  8941.056899907664\n",
            "Epoch  29  Loss  8912.335199426703\n",
            "Epoch  30  Loss  8884.700288867181\n",
            "Epoch  31  Loss  8874.720823994458\n",
            "Epoch  32  Loss  8842.272842952147\n",
            "Epoch  33  Loss  8809.914005066368\n",
            "Epoch  34  Loss  8794.37180523039\n",
            "Epoch  35  Loss  8775.367441386632\n",
            "Epoch  36  Loss  8762.428921651703\n",
            "Epoch  37  Loss  8721.743122662168\n",
            "Epoch  38  Loss  8699.946916806142\n",
            "Epoch  39  Loss  8678.007222138202\n",
            "Epoch  40  Loss  8662.740445181978\n",
            "Epoch  41  Loss  8637.608834537037\n",
            "Epoch  42  Loss  8617.64035145926\n",
            "Epoch  43  Loss  8594.201876718891\n",
            "Epoch  44  Loss  8570.655172119996\n",
            "Epoch  45  Loss  8546.91543718576\n",
            "Epoch  46  Loss  8524.420454824536\n",
            "Epoch  47  Loss  8503.135234693602\n",
            "Epoch  48  Loss  8474.633421500599\n",
            "Epoch  49  Loss  8459.247426200718\n",
            "Epoch  50  Loss  8426.966625604005\n",
            "Epoch  51  Loss  8418.562029291166\n",
            "Epoch  52  Loss  8390.504045844256\n",
            "Epoch  53  Loss  8365.261281817839\n",
            "Epoch  54  Loss  8349.081814951609\n",
            "Epoch  55  Loss  8321.858673708119\n",
            "Epoch  56  Loss  8302.360125230398\n",
            "Epoch  57  Loss  8284.97776660119\n",
            "Epoch  58  Loss  8268.718437151043\n",
            "Epoch  59  Loss  8241.696269580389\n",
            "Epoch  60  Loss  8214.019458385581\n",
            "Epoch  61  Loss  8201.35621235328\n",
            "Epoch  62  Loss  8174.832965939174\n",
            "Epoch  63  Loss  8153.221492332128\n",
            "Epoch  64  Loss  8136.685287507379\n",
            "Epoch  65  Loss  8111.254153601767\n",
            "Epoch  66  Loss  8091.786817656724\n",
            "Epoch  67  Loss  8073.2213616431745\n",
            "Epoch  68  Loss  8045.573516618719\n",
            "Epoch  69  Loss  8029.164045238743\n",
            "Epoch  70  Loss  8010.08709794274\n",
            "Epoch  71  Loss  7984.462211996663\n",
            "Epoch  72  Loss  7971.123769562335\n",
            "Epoch  73  Loss  7943.398269089898\n",
            "Epoch  74  Loss  7926.885215450203\n",
            "Epoch  75  Loss  7911.125999732655\n",
            "Epoch  76  Loss  7882.695702963114\n",
            "Epoch  77  Loss  7874.382620458477\n",
            "Epoch  78  Loss  7848.040055414776\n",
            "Epoch  79  Loss  7820.994091684399\n",
            "Epoch  80  Loss  7811.8009376406\n",
            "Epoch  81  Loss  7794.327014525416\n",
            "Epoch  82  Loss  7774.961025137444\n",
            "Epoch  83  Loss  7752.520901143283\n",
            "Epoch  84  Loss  7725.890756449391\n",
            "Epoch  85  Loss  7706.286764240017\n",
            "Epoch  86  Loss  7698.181900849481\n",
            "Epoch  87  Loss  7671.309577944761\n",
            "Epoch  88  Loss  7650.424401388678\n",
            "Epoch  89  Loss  7632.7059434553785\n",
            "Epoch  90  Loss  7606.0108338549\n",
            "Epoch  91  Loss  7593.110530776655\n",
            "Epoch  92  Loss  7578.807131141697\n",
            "Epoch  93  Loss  7553.670424299661\n",
            "Epoch  94  Loss  7530.669428054386\n",
            "Epoch  95  Loss  7520.439275683463\n",
            "Epoch  96  Loss  7500.38377023398\n",
            "Epoch  97  Loss  7478.449855525599\n",
            "Epoch  98  Loss  7456.314836682673\n",
            "Epoch  99  Loss  7443.426836132368\n",
            "Epoch  100  Loss  7431.830681329803\n",
            "Epoch  101  Loss  7410.00460020358\n",
            "Epoch  102  Loss  7383.310684364893\n",
            "Epoch  103  Loss  7373.26646387533\n",
            "Epoch  104  Loss  7350.327920596022\n",
            "Epoch  105  Loss  7339.1942966287625\n",
            "Epoch  106  Loss  7314.990258452573\n",
            "Epoch  107  Loss  7303.038172513772\n",
            "Epoch  108  Loss  7276.924203134394\n",
            "Epoch  109  Loss  7264.605427138058\n",
            "Epoch  110  Loss  7251.968201185253\n",
            "Epoch  111  Loss  7225.68890252643\n",
            "Epoch  112  Loss  7203.897831718881\n",
            "Epoch  113  Loss  7193.629394004312\n",
            "Epoch  114  Loss  7174.978228359897\n",
            "Epoch  115  Loss  7157.422930731932\n",
            "Epoch  116  Loss  7140.227997515204\n",
            "Epoch  117  Loss  7130.959659406503\n",
            "Epoch  118  Loss  7119.163419084253\n",
            "Epoch  119  Loss  7086.154892947117\n",
            "Epoch  120  Loss  7073.302588514299\n",
            "Epoch  121  Loss  7056.894436564825\n",
            "Epoch  122  Loss  7040.113009069604\n",
            "Epoch  123  Loss  7016.951713599799\n",
            "Epoch  124  Loss  7001.3160890995405\n",
            "Epoch  125  Loss  6992.355264876087\n",
            "Epoch  126  Loss  6972.564450103837\n",
            "Epoch  127  Loss  6956.3744249551055\n",
            "Epoch  128  Loss  6940.016171174063\n",
            "Epoch  129  Loss  6919.23871783061\n",
            "Epoch  130  Loss  6909.130772830522\n",
            "Epoch  131  Loss  6894.172145433317\n",
            "Epoch  132  Loss  6877.098474413698\n",
            "Epoch  133  Loss  6863.9569437710015\n",
            "Epoch  134  Loss  6838.901646732518\n",
            "Epoch  135  Loss  6821.244910870179\n",
            "Epoch  136  Loss  6809.143291437425\n",
            "Epoch  137  Loss  6791.737737610816\n",
            "Epoch  138  Loss  6784.744432990612\n",
            "Epoch  139  Loss  6764.993593802037\n",
            "Epoch  140  Loss  6744.296944241524\n",
            "Epoch  141  Loss  6723.277766747754\n",
            "Epoch  142  Loss  6713.091000867454\n",
            "Epoch  143  Loss  6699.987350640556\n",
            "Epoch  144  Loss  6680.848278943169\n",
            "Epoch  145  Loss  6667.178477173018\n",
            "Epoch  146  Loss  6647.383432003396\n",
            "Epoch  147  Loss  6637.49927438584\n",
            "Epoch  148  Loss  6625.244130914956\n",
            "Epoch  149  Loss  6603.223809997084\n",
            "Epoch  150  Loss  6585.1717723909505\n",
            "Epoch  151  Loss  6580.929246899599\n",
            "Epoch  152  Loss  6571.221317491402\n",
            "Epoch  153  Loss  6544.641011248653\n",
            "Epoch  154  Loss  6535.449306241508\n",
            "Epoch  155  Loss  6519.3520537802615\n",
            "Epoch  156  Loss  6499.4526665806525\n",
            "Epoch  157  Loss  6487.79925761045\n",
            "Epoch  158  Loss  6474.084829443285\n",
            "Epoch  159  Loss  6456.296311140158\n",
            "Epoch  160  Loss  6443.697844255523\n",
            "Epoch  161  Loss  6434.5584320306025\n",
            "Epoch  162  Loss  6420.327911970605\n",
            "Epoch  163  Loss  6400.405727981691\n",
            "Epoch  164  Loss  6385.629139792709\n",
            "Epoch  165  Loss  6372.8551388884\n",
            "Epoch  166  Loss  6355.13705673666\n",
            "Epoch  167  Loss  6345.30622549014\n",
            "Epoch  168  Loss  6331.957478408199\n",
            "Epoch  169  Loss  6312.393220955955\n",
            "Epoch  170  Loss  6299.826260868619\n",
            "Epoch  171  Loss  6293.3232574051135\n",
            "Epoch  172  Loss  6276.019520026899\n",
            "Epoch  173  Loss  6257.662905365086\n",
            "Epoch  174  Loss  6251.777251235122\n",
            "Epoch  175  Loss  6235.063945784857\n",
            "Epoch  176  Loss  6227.440522920599\n",
            "Epoch  177  Loss  6202.610143598694\n",
            "Epoch  178  Loss  6193.325528472805\n",
            "Epoch  179  Loss  6182.329325934612\n",
            "Epoch  180  Loss  6159.381759843697\n",
            "Epoch  181  Loss  6143.7334259711815\n",
            "Epoch  182  Loss  6139.8690989182\n",
            "Epoch  183  Loss  6127.322108253753\n",
            "Epoch  184  Loss  6114.427179946618\n",
            "Epoch  185  Loss  6102.38632255624\n",
            "Epoch  186  Loss  6087.476818035723\n",
            "Epoch  187  Loss  6075.797616026443\n",
            "Epoch  188  Loss  6057.248315052283\n",
            "Epoch  189  Loss  6043.966147159973\n",
            "Epoch  190  Loss  6039.882938837936\n",
            "Epoch  191  Loss  6032.635911750507\n",
            "Epoch  192  Loss  6011.62520012593\n",
            "Epoch  193  Loss  5997.476822389346\n",
            "Epoch  194  Loss  5987.153477747073\n",
            "Epoch  195  Loss  5968.828678270786\n",
            "Epoch  196  Loss  5957.267022539022\n",
            "Epoch  197  Loss  5945.647772470937\n",
            "Epoch  198  Loss  5931.02328685933\n",
            "Epoch  199  Loss  5918.550464823494\n",
            "Epoch  200  Loss  5913.928998364176\n",
            "Epoch  201  Loss  5901.557211147923\n",
            "Epoch  202  Loss  5880.038178541504\n",
            "Epoch  203  Loss  5869.250102750297\n",
            "Epoch  204  Loss  5861.3681150583125\n",
            "Epoch  205  Loss  5849.280756975787\n",
            "Epoch  206  Loss  5835.378388152713\n",
            "Epoch  207  Loss  5827.270707902467\n",
            "Epoch  208  Loss  5813.717492477411\n",
            "Epoch  209  Loss  5799.2400575721995\n",
            "Epoch  210  Loss  5790.996421717021\n",
            "Epoch  211  Loss  5771.082606735574\n",
            "Epoch  212  Loss  5769.2317781491365\n",
            "Epoch  213  Loss  5752.9625192280755\n",
            "Epoch  214  Loss  5745.361728267407\n",
            "Epoch  215  Loss  5729.243167855828\n",
            "Epoch  216  Loss  5716.618669677455\n",
            "Epoch  217  Loss  5703.182296676834\n",
            "Epoch  218  Loss  5693.340143355666\n",
            "Epoch  219  Loss  5676.470431118165\n",
            "Epoch  220  Loss  5674.104487483627\n",
            "Epoch  221  Loss  5656.0379186047285\n",
            "Epoch  222  Loss  5646.994970909313\n",
            "Epoch  223  Loss  5629.894914260408\n",
            "Epoch  224  Loss  5622.786638982538\n",
            "Epoch  225  Loss  5617.610409067015\n",
            "Epoch  226  Loss  5593.6058524004475\n",
            "Epoch  227  Loss  5593.291955333636\n",
            "Epoch  228  Loss  5573.777498671845\n",
            "Epoch  229  Loss  5567.5182575231565\n",
            "Epoch  230  Loss  5568.751760625338\n",
            "Epoch  231  Loss  5545.835357890654\n",
            "Epoch  232  Loss  5528.044418712834\n",
            "Epoch  233  Loss  5528.385929556945\n",
            "Epoch  234  Loss  5514.938329043846\n",
            "Epoch  235  Loss  5495.318136869061\n",
            "Epoch  236  Loss  5493.720388390324\n",
            "Epoch  237  Loss  5486.371891408723\n",
            "Epoch  238  Loss  5465.311823756175\n",
            "Epoch  239  Loss  5457.708406176556\n",
            "Epoch  240  Loss  5449.391013449179\n",
            "Epoch  241  Loss  5437.296823451291\n",
            "Epoch  242  Loss  5428.52545577194\n",
            "Epoch  243  Loss  5413.756019746562\n",
            "Epoch  244  Loss  5407.700727225881\n",
            "Epoch  245  Loss  5397.990410575027\n",
            "Epoch  246  Loss  5387.594923991195\n",
            "Epoch  247  Loss  5377.587793829467\n",
            "Epoch  248  Loss  5361.583906437696\n",
            "Epoch  249  Loss  5357.881903656064\n",
            "Epoch  250  Loss  5338.17782947349\n",
            "Epoch  251  Loss  5338.1143611371635\n",
            "Epoch  252  Loss  5326.022298103246\n",
            "Epoch  253  Loss  5316.157429778131\n",
            "Epoch  254  Loss  5299.252075784795\n",
            "Epoch  255  Loss  5287.986866614421\n",
            "Epoch  256  Loss  5279.179120689358\n",
            "Epoch  257  Loss  5270.992239932672\n",
            "Epoch  258  Loss  5263.782768536696\n",
            "Epoch  259  Loss  5250.523551302703\n",
            "Epoch  260  Loss  5243.474412102666\n",
            "Epoch  261  Loss  5227.568222716689\n",
            "Epoch  262  Loss  5219.42497094242\n",
            "Epoch  263  Loss  5213.931963879809\n",
            "Epoch  264  Loss  5198.787207227536\n",
            "Epoch  265  Loss  5189.370789295045\n",
            "Epoch  266  Loss  5181.024139854097\n",
            "Epoch  267  Loss  5168.360797203993\n",
            "Epoch  268  Loss  5162.2592857195095\n",
            "Epoch  269  Loss  5151.584600937238\n",
            "Epoch  270  Loss  5147.450646466775\n",
            "Epoch  271  Loss  5137.597269806677\n",
            "Epoch  272  Loss  5122.140632928566\n",
            "Epoch  273  Loss  5108.526695406263\n",
            "Epoch  274  Loss  5105.870680019787\n",
            "Epoch  275  Loss  5091.535809887613\n",
            "Epoch  276  Loss  5080.6436216987895\n",
            "Epoch  277  Loss  5081.279076655586\n",
            "Epoch  278  Loss  5059.745119763294\n",
            "Epoch  279  Loss  5059.780590174123\n",
            "Epoch  280  Loss  5048.386879306589\n",
            "Epoch  281  Loss  5035.623905415906\n",
            "Epoch  282  Loss  5025.081603350576\n",
            "Epoch  283  Loss  5022.748668334477\n",
            "Epoch  284  Loss  5007.082191063872\n",
            "Epoch  285  Loss  5000.472875890877\n",
            "Epoch  286  Loss  4988.426544754439\n",
            "Epoch  287  Loss  4975.7551422796705\n",
            "Epoch  288  Loss  4963.4980952959595\n",
            "Epoch  289  Loss  4965.029963729323\n",
            "Epoch  290  Loss  4957.041961506264\n",
            "Epoch  291  Loss  4940.839489317246\n",
            "Epoch  292  Loss  4925.061943370571\n",
            "Epoch  293  Loss  4927.399016021591\n",
            "Epoch  294  Loss  4919.217545320272\n",
            "Epoch  295  Loss  4903.567091098916\n",
            "Epoch  296  Loss  4899.440767531872\n",
            "Epoch  297  Loss  4888.880986724073\n",
            "Epoch  298  Loss  4887.885336823665\n",
            "Epoch  299  Loss  4870.953102167586\n",
            "Epoch  300  Loss  4861.914823829973\n",
            "Epoch  301  Loss  4860.184818886492\n",
            "Epoch  302  Loss  4847.701699914266\n",
            "Epoch  303  Loss  4834.500798895281\n",
            "Epoch  304  Loss  4828.545341553722\n",
            "Epoch  305  Loss  4821.182859040558\n",
            "Epoch  306  Loss  4813.791279233142\n",
            "Epoch  307  Loss  4802.427637608133\n",
            "Epoch  308  Loss  4791.827157675332\n",
            "Epoch  309  Loss  4794.940552938341\n",
            "Epoch  310  Loss  4771.8781318096435\n",
            "Epoch  311  Loss  4765.892448785446\n",
            "Epoch  312  Loss  4757.475801446266\n",
            "Epoch  313  Loss  4754.259898182472\n",
            "Epoch  314  Loss  4742.104341789191\n",
            "Epoch  315  Loss  4737.264085463446\n",
            "Epoch  316  Loss  4725.399365669164\n",
            "Epoch  317  Loss  4723.923725282972\n",
            "Epoch  318  Loss  4710.201218210099\n",
            "Epoch  319  Loss  4696.053016981575\n",
            "Epoch  320  Loss  4695.186764501454\n",
            "Epoch  321  Loss  4681.415801481791\n",
            "Epoch  322  Loss  4676.65458270262\n",
            "Epoch  323  Loss  4675.587743086019\n",
            "Epoch  324  Loss  4659.678483807954\n",
            "Epoch  325  Loss  4657.012972514182\n",
            "Epoch  326  Loss  4644.719647036043\n",
            "Epoch  327  Loss  4636.120361984322\n",
            "Epoch  328  Loss  4629.038357202139\n",
            "Epoch  329  Loss  4620.875233278719\n",
            "Epoch  330  Loss  4601.407137790393\n",
            "Epoch  331  Loss  4607.7852303288255\n",
            "Epoch  332  Loss  4593.419715770852\n",
            "Epoch  333  Loss  4587.283452461515\n",
            "Epoch  334  Loss  4592.074831368282\n",
            "Epoch  335  Loss  4572.1582232956225\n",
            "Epoch  336  Loss  4562.782180142445\n",
            "Epoch  337  Loss  4553.822200987277\n",
            "Epoch  338  Loss  4548.136580311254\n",
            "Epoch  339  Loss  4543.340835276287\n",
            "Epoch  340  Loss  4525.261093039056\n",
            "Epoch  341  Loss  4522.689191748656\n",
            "Epoch  342  Loss  4524.94147691754\n",
            "Epoch  343  Loss  4508.348272803899\n",
            "Epoch  344  Loss  4503.306653822946\n",
            "Epoch  345  Loss  4486.524633510009\n",
            "Epoch  346  Loss  4488.035131989484\n",
            "Epoch  347  Loss  4477.940574224342\n",
            "Epoch  348  Loss  4471.031659083673\n",
            "Epoch  349  Loss  4462.038013792865\n",
            "Epoch  350  Loss  4450.113591916152\n",
            "Epoch  351  Loss  4449.359034956145\n",
            "Epoch  352  Loss  4443.39214111153\n",
            "Epoch  353  Loss  4424.555548926295\n",
            "Epoch  354  Loss  4420.490954791446\n",
            "Epoch  355  Loss  4419.288739723136\n",
            "Epoch  356  Loss  4405.450523490608\n",
            "Epoch  357  Loss  4406.783253998228\n",
            "Epoch  358  Loss  4393.9838994715865\n",
            "Epoch  359  Loss  4387.693636888363\n",
            "Epoch  360  Loss  4379.752572700843\n",
            "Epoch  361  Loss  4369.132207075907\n",
            "Epoch  362  Loss  4367.0108387094\n",
            "Epoch  363  Loss  4362.494734952645\n",
            "Epoch  364  Loss  4350.081741340305\n",
            "Epoch  365  Loss  4346.595749717574\n",
            "Epoch  366  Loss  4327.784234498951\n",
            "Epoch  367  Loss  4325.196857307497\n",
            "Epoch  368  Loss  4323.849649036072\n",
            "Epoch  369  Loss  4309.893916701697\n",
            "Epoch  370  Loss  4306.07970253759\n",
            "Epoch  371  Loss  4299.807575966462\n",
            "Epoch  372  Loss  4295.5569385826175\n",
            "Epoch  373  Loss  4289.3231670084115\n",
            "Epoch  374  Loss  4275.122028837635\n",
            "Epoch  375  Loss  4272.31604618563\n",
            "Epoch  376  Loss  4263.903465150598\n",
            "Epoch  377  Loss  4255.6590843711465\n",
            "Epoch  378  Loss  4248.558940611213\n",
            "Epoch  379  Loss  4238.196834661153\n",
            "Epoch  380  Loss  4232.632753753975\n",
            "Epoch  381  Loss  4231.37858703557\n",
            "Epoch  382  Loss  4226.1314433770585\n",
            "Epoch  383  Loss  4211.69688148045\n",
            "Epoch  384  Loss  4207.523744852853\n",
            "Epoch  385  Loss  4199.272689801615\n",
            "Epoch  386  Loss  4194.066299268043\n",
            "Epoch  387  Loss  4193.891902431045\n",
            "Epoch  388  Loss  4177.293467749825\n",
            "Epoch  389  Loss  4169.458933667515\n",
            "Epoch  390  Loss  4162.48125522341\n",
            "Epoch  391  Loss  4155.146938780029\n",
            "Epoch  392  Loss  4157.384098688848\n",
            "Epoch  393  Loss  4149.632674261918\n",
            "Epoch  394  Loss  4133.43232946061\n",
            "Epoch  395  Loss  4133.908343255056\n",
            "Epoch  396  Loss  4129.2089638197285\n",
            "Epoch  397  Loss  4121.105732191878\n",
            "Epoch  398  Loss  4108.270528045667\n",
            "Epoch  399  Loss  4108.883733639155\n",
            "Epoch  400  Loss  4092.711060729993\n",
            "Epoch  401  Loss  4092.2421500842775\n",
            "Epoch  402  Loss  4090.898274523457\n",
            "Epoch  403  Loss  4080.6077938301346\n",
            "Epoch  404  Loss  4071.100738704685\n",
            "Epoch  405  Loss  4069.9076879360355\n",
            "Epoch  406  Loss  4064.419984870407\n",
            "Epoch  407  Loss  4045.808748791201\n",
            "Epoch  408  Loss  4042.037016256567\n",
            "Epoch  409  Loss  4044.115335620578\n",
            "Epoch  410  Loss  4035.225944568557\n",
            "Epoch  411  Loss  4033.4793903467057\n",
            "Epoch  412  Loss  4016.902658449755\n",
            "Epoch  413  Loss  4010.994120561746\n",
            "Epoch  414  Loss  4006.2858970095867\n",
            "Epoch  415  Loss  3997.830276999517\n",
            "Epoch  416  Loss  3997.4965283148504\n",
            "Epoch  417  Loss  3991.389875702038\n",
            "Epoch  418  Loss  3988.4142268384508\n",
            "Epoch  419  Loss  3974.4588478558508\n",
            "Epoch  420  Loss  3975.2665714592795\n",
            "Epoch  421  Loss  3963.0947125374023\n",
            "Epoch  422  Loss  3957.9854422672342\n",
            "Epoch  423  Loss  3947.002121313069\n",
            "Epoch  424  Loss  3945.3704214678514\n",
            "Epoch  425  Loss  3937.9512324926714\n",
            "Epoch  426  Loss  3937.493266466195\n",
            "Epoch  427  Loss  3925.2928752589055\n",
            "Epoch  428  Loss  3913.3384500579896\n",
            "Epoch  429  Loss  3911.7168072725126\n",
            "Epoch  430  Loss  3907.0036600813673\n",
            "Epoch  431  Loss  3906.3344577232137\n",
            "Epoch  432  Loss  3895.6162087666826\n",
            "Epoch  433  Loss  3895.9199001876723\n",
            "Epoch  434  Loss  3885.5047416671378\n",
            "Epoch  435  Loss  3879.302055884261\n",
            "Epoch  436  Loss  3869.3790079021182\n",
            "Epoch  437  Loss  3866.307638424768\n",
            "Epoch  438  Loss  3862.569938445453\n",
            "Epoch  439  Loss  3854.9728193375595\n",
            "Epoch  440  Loss  3849.1662104768593\n",
            "Epoch  441  Loss  3848.4655207821475\n",
            "Epoch  442  Loss  3834.6246897216247\n",
            "Epoch  443  Loss  3832.82111601107\n",
            "Epoch  444  Loss  3819.413079069064\n",
            "Epoch  445  Loss  3816.2764055471243\n",
            "Epoch  446  Loss  3813.5770633920065\n",
            "Epoch  447  Loss  3802.928137754609\n",
            "Epoch  448  Loss  3800.7098703542065\n",
            "Epoch  449  Loss  3796.7998109622836\n",
            "Epoch  450  Loss  3792.818322268352\n",
            "Epoch  451  Loss  3780.1098752616613\n",
            "Epoch  452  Loss  3778.6408557451427\n",
            "Epoch  453  Loss  3771.2675643422626\n",
            "Epoch  454  Loss  3768.026207495068\n",
            "Epoch  455  Loss  3764.5562000761984\n",
            "Epoch  456  Loss  3755.984392469218\n",
            "Epoch  457  Loss  3750.4074880039943\n",
            "Epoch  458  Loss  3745.827638493973\n",
            "Epoch  459  Loss  3740.2521242912326\n",
            "Epoch  460  Loss  3730.410027024068\n",
            "Epoch  461  Loss  3721.1834272099804\n",
            "Epoch  462  Loss  3720.9556241896103\n",
            "Epoch  463  Loss  3711.4077476051143\n",
            "Epoch  464  Loss  3711.1281186997817\n",
            "Epoch  465  Loss  3710.5309069044915\n",
            "Epoch  466  Loss  3693.810835314898\n",
            "Epoch  467  Loss  3694.257936908437\n",
            "Epoch  468  Loss  3685.23690811274\n",
            "Epoch  469  Loss  3684.41759721652\n",
            "Epoch  470  Loss  3677.691802364015\n",
            "Epoch  471  Loss  3670.646577856192\n",
            "Epoch  472  Loss  3662.4815129893677\n",
            "Epoch  473  Loss  3656.5511525817737\n",
            "Epoch  474  Loss  3657.16630335189\n",
            "Epoch  475  Loss  3648.4017556481454\n",
            "Epoch  476  Loss  3644.324315533545\n",
            "Epoch  477  Loss  3637.997712275128\n",
            "Epoch  478  Loss  3632.5382412955937\n",
            "Epoch  479  Loss  3619.80751250614\n",
            "Epoch  480  Loss  3617.990164393886\n",
            "Epoch  481  Loss  3616.4288957671138\n",
            "Epoch  482  Loss  3616.4680701020084\n",
            "Epoch  483  Loss  3614.961307572022\n",
            "Epoch  484  Loss  3594.367527690835\n",
            "Epoch  485  Loss  3591.0498846053815\n",
            "Epoch  486  Loss  3588.5127787495558\n",
            "Epoch  487  Loss  3582.99465842103\n",
            "Epoch  488  Loss  3578.172236383018\n",
            "Epoch  489  Loss  3566.830198587917\n",
            "Epoch  490  Loss  3569.463030732644\n",
            "Epoch  491  Loss  3559.841672901248\n",
            "Epoch  492  Loss  3557.9194889701866\n",
            "Epoch  493  Loss  3550.72329837785\n",
            "Epoch  494  Loss  3551.8309260510505\n",
            "Epoch  495  Loss  3544.7437618347867\n",
            "Epoch  496  Loss  3542.5616542713483\n",
            "Epoch  497  Loss  3534.940291081968\n",
            "Epoch  498  Loss  3518.798823972562\n",
            "Epoch  499  Loss  3518.1831317202677\n",
            "Epoch  500  Loss  3512.7708345208116\n",
            "Epoch  501  Loss  3515.276510888329\n",
            "Epoch  502  Loss  3497.991399319124\n",
            "Epoch  503  Loss  3500.2343271607047\n",
            "Epoch  504  Loss  3495.7557224258567\n",
            "Epoch  505  Loss  3492.546526231178\n",
            "Epoch  506  Loss  3481.340771783651\n",
            "Epoch  507  Loss  3477.222526698836\n",
            "Epoch  508  Loss  3471.270078915751\n",
            "Epoch  509  Loss  3476.42174996716\n",
            "Epoch  510  Loss  3469.2635837877556\n",
            "Epoch  511  Loss  3459.1387536426373\n",
            "Epoch  512  Loss  3456.431435726399\n",
            "Epoch  513  Loss  3447.5710310625204\n",
            "Epoch  514  Loss  3446.069913622036\n",
            "Epoch  515  Loss  3437.157490071613\n",
            "Epoch  516  Loss  3431.6432701670615\n",
            "Epoch  517  Loss  3430.136075938851\n",
            "Epoch  518  Loss  3430.3477424202533\n",
            "Epoch  519  Loss  3421.9914600452057\n",
            "Epoch  520  Loss  3413.514085788156\n",
            "Epoch  521  Loss  3407.331639060656\n",
            "Epoch  522  Loss  3406.533499012913\n",
            "Epoch  523  Loss  3402.86272944432\n",
            "Epoch  524  Loss  3394.050902282073\n",
            "Epoch  525  Loss  3387.502020469339\n",
            "Epoch  526  Loss  3387.349214992427\n",
            "Epoch  527  Loss  3381.930904862724\n",
            "Epoch  528  Loss  3373.7384122693975\n",
            "Epoch  529  Loss  3371.846975037009\n",
            "Epoch  530  Loss  3364.290432400058\n",
            "Epoch  531  Loss  3364.885243431599\n",
            "Epoch  532  Loss  3355.4356627810243\n",
            "Epoch  533  Loss  3357.9436644851094\n",
            "Epoch  534  Loss  3346.8745420971663\n",
            "Epoch  535  Loss  3341.1959605768807\n",
            "Epoch  536  Loss  3335.676971229671\n",
            "Epoch  537  Loss  3330.993586905456\n",
            "Epoch  538  Loss  3322.123376534894\n",
            "Epoch  539  Loss  3325.1947534646897\n",
            "Epoch  540  Loss  3317.628885952725\n",
            "Epoch  541  Loss  3317.317952094174\n",
            "Epoch  542  Loss  3305.8967124072724\n",
            "Epoch  543  Loss  3309.0408289581396\n",
            "Epoch  544  Loss  3295.790284106373\n",
            "Epoch  545  Loss  3295.530575151961\n",
            "Epoch  546  Loss  3290.8611607135263\n",
            "Epoch  547  Loss  3283.9612635614963\n",
            "Epoch  548  Loss  3279.7230061097034\n",
            "Epoch  549  Loss  3277.777593511297\n",
            "Epoch  550  Loss  3277.20190101954\n",
            "Epoch  551  Loss  3270.2290256832252\n",
            "Epoch  552  Loss  3263.873538700442\n",
            "Epoch  553  Loss  3254.876160976714\n",
            "Epoch  554  Loss  3256.405930740225\n",
            "Epoch  555  Loss  3255.6737099605957\n",
            "Epoch  556  Loss  3245.8194964534355\n",
            "Epoch  557  Loss  3245.4893031218676\n",
            "Epoch  558  Loss  3239.3699377737244\n",
            "Epoch  559  Loss  3232.7844095773457\n",
            "Epoch  560  Loss  3231.562191654252\n",
            "Epoch  561  Loss  3225.21264865415\n",
            "Epoch  562  Loss  3222.9681318189005\n",
            "Epoch  563  Loss  3217.9007052908764\n",
            "Epoch  564  Loss  3210.7164665469218\n",
            "Epoch  565  Loss  3205.157656640299\n",
            "Epoch  566  Loss  3196.0545603583655\n",
            "Epoch  567  Loss  3197.565570660865\n",
            "Epoch  568  Loss  3201.2067652568253\n",
            "Epoch  569  Loss  3184.6476103142486\n",
            "Epoch  570  Loss  3180.271774899717\n",
            "Epoch  571  Loss  3171.3036065547903\n",
            "Epoch  572  Loss  3176.453592123987\n",
            "Epoch  573  Loss  3172.3904508681126\n",
            "Epoch  574  Loss  3170.68409605739\n",
            "Epoch  575  Loss  3158.8889374486052\n",
            "Epoch  576  Loss  3158.3341201988624\n",
            "Epoch  577  Loss  3154.31223531557\n",
            "Epoch  578  Loss  3149.480314374377\n",
            "Epoch  579  Loss  3138.6840324302825\n",
            "Epoch  580  Loss  3136.1452868065885\n",
            "Epoch  581  Loss  3131.3752027554074\n",
            "Epoch  582  Loss  3135.053593828014\n",
            "Epoch  583  Loss  3129.572564316995\n",
            "Epoch  584  Loss  3123.994339730932\n",
            "Epoch  585  Loss  3121.9328422624567\n",
            "Epoch  586  Loss  3110.948225225414\n",
            "Epoch  587  Loss  3115.6947129107934\n",
            "Epoch  588  Loss  3108.204105859833\n",
            "Epoch  589  Loss  3098.472074653301\n",
            "Epoch  590  Loss  3102.027830915351\n",
            "Epoch  591  Loss  3096.553531783284\n",
            "Epoch  592  Loss  3090.6511302082276\n",
            "Epoch  593  Loss  3081.338525789034\n",
            "Epoch  594  Loss  3081.5910895819675\n",
            "Epoch  595  Loss  3078.4820806740704\n",
            "Epoch  596  Loss  3074.662421490622\n",
            "Epoch  597  Loss  3068.663032702172\n",
            "Epoch  598  Loss  3061.0376600897985\n",
            "Epoch  599  Loss  3062.6670798796713\n",
            "Epoch  600  Loss  3057.884456187328\n",
            "Epoch  601  Loss  3053.6643645902495\n",
            "Epoch  602  Loss  3046.6443362151977\n",
            "Epoch  603  Loss  3048.559841726204\n",
            "Epoch  604  Loss  3040.9200278890803\n",
            "Epoch  605  Loss  3033.0631537627028\n",
            "Epoch  606  Loss  3031.5001829975795\n",
            "Epoch  607  Loss  3032.0352573939435\n",
            "Epoch  608  Loss  3024.187023652253\n",
            "Epoch  609  Loss  3020.92060410851\n",
            "Epoch  610  Loss  3017.747175831436\n",
            "Epoch  611  Loss  3011.7537721362105\n",
            "Epoch  612  Loss  3014.9232526806018\n",
            "Epoch  613  Loss  3006.014245622877\n",
            "Epoch  614  Loss  3003.310507427622\n",
            "Epoch  615  Loss  2996.708105484178\n",
            "Epoch  616  Loss  2992.5471719428315\n",
            "Epoch  617  Loss  2989.216492771337\n",
            "Epoch  618  Loss  2982.623611523164\n",
            "Epoch  619  Loss  2984.0374238141526\n",
            "Epoch  620  Loss  2980.4341295015324\n",
            "Epoch  621  Loss  2972.1766738855117\n",
            "Epoch  622  Loss  2967.501283275314\n",
            "Epoch  623  Loss  2960.2143982971834\n",
            "Epoch  624  Loss  2965.494634499819\n",
            "Epoch  625  Loss  2958.17760049471\n",
            "Epoch  626  Loss  2950.58800111258\n",
            "Epoch  627  Loss  2955.4112380307006\n",
            "Epoch  628  Loss  2945.42221049895\n",
            "Epoch  629  Loss  2936.6389359965287\n",
            "Epoch  630  Loss  2938.0864163130454\n",
            "Epoch  631  Loss  2934.761697544265\n",
            "Epoch  632  Loss  2928.847402642734\n",
            "Epoch  633  Loss  2923.3538579579863\n",
            "Epoch  634  Loss  2926.7785491349186\n",
            "Epoch  635  Loss  2920.12985302096\n",
            "Epoch  636  Loss  2912.5873980771685\n",
            "Epoch  637  Loss  2907.731111265105\n",
            "Epoch  638  Loss  2910.5990112644513\n",
            "Epoch  639  Loss  2910.7829886581226\n",
            "Epoch  640  Loss  2900.474762656093\n",
            "Epoch  641  Loss  2895.367924044604\n",
            "Epoch  642  Loss  2888.168493116858\n",
            "Epoch  643  Loss  2891.4049180796083\n",
            "Epoch  644  Loss  2888.673396302729\n",
            "Epoch  645  Loss  2882.601639872839\n",
            "Epoch  646  Loss  2886.5939517958695\n",
            "Epoch  647  Loss  2867.8040105780874\n",
            "Epoch  648  Loss  2867.9357590892305\n",
            "Epoch  649  Loss  2865.809778206986\n",
            "Epoch  650  Loss  2864.1525086216434\n",
            "Epoch  651  Loss  2859.837297781746\n",
            "Epoch  652  Loss  2854.945600476026\n",
            "Epoch  653  Loss  2854.82262047576\n",
            "Epoch  654  Loss  2853.0574698012324\n",
            "Epoch  655  Loss  2846.17999819586\n",
            "Epoch  656  Loss  2843.103908707689\n",
            "Epoch  657  Loss  2832.564513810428\n",
            "Epoch  658  Loss  2832.4426409266234\n",
            "Epoch  659  Loss  2830.116622501065\n",
            "Epoch  660  Loss  2821.4292983419828\n",
            "Epoch  661  Loss  2823.672157300628\n",
            "Epoch  662  Loss  2817.673473558297\n",
            "Epoch  663  Loss  2816.558964201459\n",
            "Epoch  664  Loss  2808.820111319152\n",
            "Epoch  665  Loss  2809.2133691232375\n",
            "Epoch  666  Loss  2802.7180712960885\n",
            "Epoch  667  Loss  2800.0325631669252\n",
            "Epoch  668  Loss  2798.5637111098313\n",
            "Epoch  669  Loss  2788.902204856763\n",
            "Epoch  670  Loss  2787.106187924415\n",
            "Epoch  671  Loss  2784.522586947599\n",
            "Epoch  672  Loss  2786.1435672714665\n",
            "Epoch  673  Loss  2779.8787754448176\n",
            "Epoch  674  Loss  2781.6722021575897\n",
            "Epoch  675  Loss  2774.466976911094\n",
            "Epoch  676  Loss  2772.453948493016\n",
            "Epoch  677  Loss  2766.8495014538316\n",
            "Epoch  678  Loss  2763.6342121749194\n",
            "Epoch  679  Loss  2762.629463632049\n",
            "Epoch  680  Loss  2756.987792523641\n",
            "Epoch  681  Loss  2753.4304612966685\n",
            "Epoch  682  Loss  2746.9348765077575\n",
            "Epoch  683  Loss  2748.954197245782\n",
            "Epoch  684  Loss  2735.305597024629\n",
            "Epoch  685  Loss  2742.966189215981\n",
            "Epoch  686  Loss  2732.7620576638833\n",
            "Epoch  687  Loss  2731.474367890434\n",
            "Epoch  688  Loss  2730.195627446806\n",
            "Epoch  689  Loss  2724.64279425452\n",
            "Epoch  690  Loss  2720.514884692819\n",
            "Epoch  691  Loss  2721.0946362929717\n",
            "Epoch  692  Loss  2715.291872634085\n",
            "Epoch  693  Loss  2707.2157480505944\n",
            "Epoch  694  Loss  2716.19233172193\n",
            "Epoch  695  Loss  2702.732277099968\n",
            "Epoch  696  Loss  2703.7675866336667\n",
            "Epoch  697  Loss  2696.562724871687\n",
            "Epoch  698  Loss  2693.0541887154327\n",
            "Epoch  699  Loss  2693.8150473035803\n",
            "Epoch  700  Loss  2691.73969343279\n",
            "Epoch  701  Loss  2681.47904781045\n",
            "Epoch  702  Loss  2681.962338519106\n",
            "Epoch  703  Loss  2686.6323077335664\n",
            "Epoch  704  Loss  2675.47474480848\n",
            "Epoch  705  Loss  2670.166593378335\n",
            "Epoch  706  Loss  2667.5755393268923\n",
            "Epoch  707  Loss  2662.910029606116\n",
            "Epoch  708  Loss  2668.3941000224236\n",
            "Epoch  709  Loss  2655.6307174358426\n",
            "Epoch  710  Loss  2655.817851333316\n",
            "Epoch  711  Loss  2656.833917542484\n",
            "Epoch  712  Loss  2646.046324525998\n",
            "Epoch  713  Loss  2640.97312283871\n",
            "Epoch  714  Loss  2638.9505291065893\n",
            "Epoch  715  Loss  2637.267436715387\n",
            "Epoch  716  Loss  2635.8709725652143\n",
            "Epoch  717  Loss  2633.0846122312228\n",
            "Epoch  718  Loss  2627.8844059343332\n",
            "Epoch  719  Loss  2624.3984148844065\n",
            "Epoch  720  Loss  2619.408721776327\n",
            "Epoch  721  Loss  2621.3865444747817\n",
            "Epoch  722  Loss  2614.4553016160157\n",
            "Epoch  723  Loss  2612.974295260558\n",
            "Epoch  724  Loss  2612.666934045551\n",
            "Epoch  725  Loss  2609.2904438915793\n",
            "Epoch  726  Loss  2604.3769569678425\n",
            "Epoch  727  Loss  2599.212781262701\n",
            "Epoch  728  Loss  2595.065247582744\n",
            "Epoch  729  Loss  2594.4816855362715\n",
            "Epoch  730  Loss  2593.1617230267975\n",
            "Epoch  731  Loss  2588.2537369778256\n",
            "Epoch  732  Loss  2586.7532831888607\n",
            "Epoch  733  Loss  2582.640671086484\n",
            "Epoch  734  Loss  2581.897070697462\n",
            "Epoch  735  Loss  2582.661169653855\n",
            "Epoch  736  Loss  2573.608941874196\n",
            "Epoch  737  Loss  2566.501086253158\n",
            "Epoch  738  Loss  2563.6065719115604\n",
            "Epoch  739  Loss  2559.055675114646\n",
            "Epoch  740  Loss  2559.9160156416788\n",
            "Epoch  741  Loss  2559.576066431118\n",
            "Epoch  742  Loss  2555.518138958467\n",
            "Epoch  743  Loss  2548.5695963537846\n",
            "Epoch  744  Loss  2542.6032621297927\n",
            "Epoch  745  Loss  2543.9160526847395\n",
            "Epoch  746  Loss  2539.2742919411094\n",
            "Epoch  747  Loss  2538.363363399159\n",
            "Epoch  748  Loss  2539.811948982903\n",
            "Epoch  749  Loss  2534.978488140833\n",
            "Epoch  750  Loss  2527.4063492420673\n",
            "Epoch  751  Loss  2519.0353025667555\n",
            "Epoch  752  Loss  2523.1016690545694\n",
            "Epoch  753  Loss  2517.454764461921\n",
            "Epoch  754  Loss  2517.4181988128857\n",
            "Epoch  755  Loss  2511.3541185076992\n",
            "Epoch  756  Loss  2510.928994377481\n",
            "Epoch  757  Loss  2501.854015236719\n",
            "Epoch  758  Loss  2506.302219586842\n",
            "Epoch  759  Loss  2502.551315147086\n",
            "Epoch  760  Loss  2498.405197951943\n",
            "Epoch  761  Loss  2498.1991186119817\n",
            "Epoch  762  Loss  2493.3526324967834\n",
            "Epoch  763  Loss  2490.105529204013\n",
            "Epoch  764  Loss  2487.551832163394\n",
            "Epoch  765  Loss  2483.470861547256\n",
            "Epoch  766  Loss  2477.050792590372\n",
            "Epoch  767  Loss  2477.3590053855955\n",
            "Epoch  768  Loss  2475.2200244028463\n",
            "Epoch  769  Loss  2475.54809533307\n",
            "Epoch  770  Loss  2473.550538498386\n",
            "Epoch  771  Loss  2457.911625998286\n",
            "Epoch  772  Loss  2465.9191796445734\n",
            "Epoch  773  Loss  2462.582281159319\n",
            "Epoch  774  Loss  2457.960023548779\n",
            "Epoch  775  Loss  2459.2422592427856\n",
            "Epoch  776  Loss  2452.916096812536\n",
            "Epoch  777  Loss  2446.1718505205786\n",
            "Epoch  778  Loss  2448.1683510672055\n",
            "Epoch  779  Loss  2441.5297585894036\n",
            "Epoch  780  Loss  2436.3461980662037\n",
            "Epoch  781  Loss  2441.4031444721036\n",
            "Epoch  782  Loss  2436.09017860422\n",
            "Epoch  783  Loss  2432.925436643519\n",
            "Epoch  784  Loss  2432.068250066906\n",
            "Epoch  785  Loss  2425.7530292276965\n",
            "Epoch  786  Loss  2424.147872083059\n",
            "Epoch  787  Loss  2420.5182854232444\n",
            "Epoch  788  Loss  2420.549143036885\n",
            "Epoch  789  Loss  2414.8917165569374\n",
            "Epoch  790  Loss  2409.8263482407706\n",
            "Epoch  791  Loss  2405.7436614669173\n",
            "Epoch  792  Loss  2412.0434791214693\n",
            "Epoch  793  Loss  2406.4109559145795\n",
            "Epoch  794  Loss  2406.9053401527453\n",
            "Epoch  795  Loss  2397.6857795162855\n",
            "Epoch  796  Loss  2392.1853056630243\n",
            "Epoch  797  Loss  2399.997791264744\n",
            "Epoch  798  Loss  2393.35625134304\n",
            "Epoch  799  Loss  2387.9327231338234\n",
            "Epoch  800  Loss  2387.299756047634\n",
            "Epoch  801  Loss  2388.0082147710064\n",
            "Epoch  802  Loss  2381.657331355919\n",
            "Epoch  803  Loss  2375.0776383579387\n",
            "Epoch  804  Loss  2376.404661946584\n",
            "Epoch  805  Loss  2367.026814246148\n",
            "Epoch  806  Loss  2368.5634953217\n",
            "Epoch  807  Loss  2366.2053587093005\n",
            "Epoch  808  Loss  2357.492269180596\n",
            "Epoch  809  Loss  2363.2772587430754\n",
            "Epoch  810  Loss  2355.8776873127167\n",
            "Epoch  811  Loss  2350.9467472608567\n",
            "Epoch  812  Loss  2353.662506847796\n",
            "Epoch  813  Loss  2347.2634394698202\n",
            "Epoch  814  Loss  2341.8679067140265\n",
            "Epoch  815  Loss  2345.9910462253456\n",
            "Epoch  816  Loss  2341.115724621713\n",
            "Epoch  817  Loss  2342.033934961041\n",
            "Epoch  818  Loss  2338.101106024746\n",
            "Epoch  819  Loss  2331.798564364666\n",
            "Epoch  820  Loss  2329.2929428888647\n",
            "Epoch  821  Loss  2326.786128860205\n",
            "Epoch  822  Loss  2321.6296651996313\n",
            "Epoch  823  Loss  2315.4864279122476\n",
            "Epoch  824  Loss  2318.007816405908\n",
            "Epoch  825  Loss  2322.028005650272\n",
            "Epoch  826  Loss  2316.750415185938\n",
            "Epoch  827  Loss  2312.5936526316636\n",
            "Epoch  828  Loss  2310.4677972925183\n",
            "Epoch  829  Loss  2307.171411401963\n",
            "Epoch  830  Loss  2303.5421686262393\n",
            "Epoch  831  Loss  2294.962301550577\n",
            "Epoch  832  Loss  2296.5225275980188\n",
            "Epoch  833  Loss  2299.2290880636106\n",
            "Epoch  834  Loss  2291.4397275246856\n",
            "Epoch  835  Loss  2290.2118215158284\n",
            "Epoch  836  Loss  2287.347286048719\n",
            "Epoch  837  Loss  2287.152880429277\n",
            "Epoch  838  Loss  2281.617338110961\n",
            "Epoch  839  Loss  2282.094498981591\n",
            "Epoch  840  Loss  2279.8072664829238\n",
            "Epoch  841  Loss  2276.855130432375\n",
            "Epoch  842  Loss  2270.9115322669163\n",
            "Epoch  843  Loss  2270.3622861946355\n",
            "Epoch  844  Loss  2264.8483375020814\n",
            "Epoch  845  Loss  2272.445142798874\n",
            "Epoch  846  Loss  2262.6850434876737\n",
            "Epoch  847  Loss  2260.119366253476\n",
            "Epoch  848  Loss  2256.4816455572645\n",
            "Epoch  849  Loss  2252.0868279407186\n",
            "Epoch  850  Loss  2251.915737543003\n",
            "Epoch  851  Loss  2247.788794055078\n",
            "Epoch  852  Loss  2250.344324058515\n",
            "Epoch  853  Loss  2250.7936421407508\n",
            "Epoch  854  Loss  2239.8183129695385\n",
            "Epoch  855  Loss  2240.024472597437\n",
            "Epoch  856  Loss  2234.4213108292597\n",
            "Epoch  857  Loss  2236.8987359313924\n",
            "Epoch  858  Loss  2233.2198996493717\n",
            "Epoch  859  Loss  2228.28373598958\n",
            "Epoch  860  Loss  2233.765088579762\n",
            "Epoch  861  Loss  2228.5765711798304\n",
            "Epoch  862  Loss  2223.354500068115\n",
            "Epoch  863  Loss  2218.800340244102\n",
            "Epoch  864  Loss  2219.6858568609796\n",
            "Epoch  865  Loss  2213.425656917659\n",
            "Epoch  866  Loss  2214.4555289165164\n",
            "Epoch  867  Loss  2211.8304177649215\n",
            "Epoch  868  Loss  2210.8822089360865\n",
            "Epoch  869  Loss  2200.138298305694\n",
            "Epoch  870  Loss  2203.8303123127\n",
            "Epoch  871  Loss  2201.7795677648933\n",
            "Epoch  872  Loss  2202.378925017539\n",
            "Epoch  873  Loss  2195.116119552072\n",
            "Epoch  874  Loss  2193.7365338168747\n",
            "Epoch  875  Loss  2192.1813617752687\n",
            "Epoch  876  Loss  2186.458726793548\n",
            "Epoch  877  Loss  2185.5865323144503\n",
            "Epoch  878  Loss  2185.03318388108\n",
            "Epoch  879  Loss  2182.496308376366\n",
            "Epoch  880  Loss  2179.8820966008357\n",
            "Epoch  881  Loss  2179.4147331043905\n",
            "Epoch  882  Loss  2175.2656499401664\n",
            "Epoch  883  Loss  2167.833534208408\n",
            "Epoch  884  Loss  2168.1731771736754\n",
            "Epoch  885  Loss  2167.980908459662\n",
            "Epoch  886  Loss  2163.1851964315474\n",
            "Epoch  887  Loss  2159.8450026607916\n",
            "Epoch  888  Loss  2161.5561247543\n",
            "Epoch  889  Loss  2158.981767518124\n",
            "Epoch  890  Loss  2153.8938743032577\n",
            "Epoch  891  Loss  2152.6340907003437\n",
            "Epoch  892  Loss  2149.62118678035\n",
            "Epoch  893  Loss  2147.5554686982964\n",
            "Epoch  894  Loss  2141.9284108450283\n",
            "Epoch  895  Loss  2140.0627823433665\n",
            "Epoch  896  Loss  2142.7147317144418\n",
            "Epoch  897  Loss  2144.7052112464157\n",
            "Epoch  898  Loss  2134.1209657473355\n",
            "Epoch  899  Loss  2133.388994852528\n",
            "Epoch  900  Loss  2135.3148222760533\n",
            "Epoch  901  Loss  2126.5643151459562\n",
            "Epoch  902  Loss  2129.2204108745746\n",
            "Epoch  903  Loss  2127.6448039724487\n",
            "Epoch  904  Loss  2120.5731292539062\n",
            "Epoch  905  Loss  2122.5711189574404\n",
            "Epoch  906  Loss  2119.143618284899\n",
            "Epoch  907  Loss  2117.1275699593066\n",
            "Epoch  908  Loss  2117.3379083162513\n",
            "Epoch  909  Loss  2111.7230018853393\n",
            "Epoch  910  Loss  2111.770234364534\n",
            "Epoch  911  Loss  2102.487496513766\n",
            "Epoch  912  Loss  2103.030585061626\n",
            "Epoch  913  Loss  2101.348760875624\n",
            "Epoch  914  Loss  2106.2562928625202\n",
            "Epoch  915  Loss  2098.328356407594\n",
            "Epoch  916  Loss  2098.1795810955373\n",
            "Epoch  917  Loss  2096.90637556499\n",
            "Epoch  918  Loss  2093.3267187448714\n",
            "Epoch  919  Loss  2091.0393622807915\n",
            "Epoch  920  Loss  2084.7913857798935\n",
            "Epoch  921  Loss  2086.8177196665824\n",
            "Epoch  922  Loss  2086.0208447802047\n",
            "Epoch  923  Loss  2079.745559239456\n",
            "Epoch  924  Loss  2073.555313378251\n",
            "Epoch  925  Loss  2079.3863595555204\n",
            "Epoch  926  Loss  2074.458935426581\n",
            "Epoch  927  Loss  2071.045759274279\n",
            "Epoch  928  Loss  2072.4157700109295\n",
            "Epoch  929  Loss  2066.344272033946\n",
            "Epoch  930  Loss  2066.9266163794327\n",
            "Epoch  931  Loss  2063.8975188511613\n",
            "Epoch  932  Loss  2060.9702430232037\n",
            "Epoch  933  Loss  2054.3363100962165\n",
            "Epoch  934  Loss  2055.9050281731575\n",
            "Epoch  935  Loss  2052.4196403994392\n",
            "Epoch  936  Loss  2054.5010377800777\n",
            "Epoch  937  Loss  2052.0617882808974\n",
            "Epoch  938  Loss  2048.9525696657643\n",
            "Epoch  939  Loss  2044.7261765301919\n",
            "Epoch  940  Loss  2042.7065874295965\n",
            "Epoch  941  Loss  2038.8050892753408\n",
            "Epoch  942  Loss  2041.0961823133127\n",
            "Epoch  943  Loss  2039.9341786235127\n",
            "Epoch  944  Loss  2034.185547914282\n",
            "Epoch  945  Loss  2033.518226017374\n",
            "Epoch  946  Loss  2029.767082620895\n",
            "Epoch  947  Loss  2030.3243037579668\n",
            "Epoch  948  Loss  2026.7810666000505\n",
            "Epoch  949  Loss  2025.0518511055368\n",
            "Epoch  950  Loss  2022.1781429723892\n",
            "Epoch  951  Loss  2017.3341636047905\n",
            "Epoch  952  Loss  2013.1835976819684\n",
            "Epoch  953  Loss  2014.8972297295275\n",
            "Epoch  954  Loss  2011.0076949318538\n",
            "Epoch  955  Loss  2014.2485970018795\n",
            "Epoch  956  Loss  2002.3779052310115\n",
            "Epoch  957  Loss  2006.0293645763645\n",
            "Epoch  958  Loss  2001.6084094375253\n",
            "Epoch  959  Loss  2001.4291533699354\n",
            "Epoch  960  Loss  2003.334425352753\n",
            "Epoch  961  Loss  1995.7208948741277\n",
            "Epoch  962  Loss  1996.8275871662547\n",
            "Epoch  963  Loss  1995.4835329666898\n",
            "Epoch  964  Loss  1995.305580955888\n",
            "Epoch  965  Loss  1991.2759208074601\n",
            "Epoch  966  Loss  1987.7930694795857\n",
            "Epoch  967  Loss  1985.881815497936\n",
            "Epoch  968  Loss  1986.9419037847183\n",
            "Epoch  969  Loss  1976.808161797949\n",
            "Epoch  970  Loss  1982.6607553967037\n",
            "Epoch  971  Loss  1982.9227138643855\n",
            "Epoch  972  Loss  1979.1170378402373\n",
            "Epoch  973  Loss  1973.3766358577907\n",
            "Epoch  974  Loss  1967.7130453863142\n",
            "Epoch  975  Loss  1970.0837939585929\n",
            "Epoch  976  Loss  1968.3401882386888\n",
            "Epoch  977  Loss  1970.0759449184943\n",
            "Epoch  978  Loss  1959.312655911092\n",
            "Epoch  979  Loss  1966.9603611643415\n",
            "Epoch  980  Loss  1966.913488976241\n",
            "Epoch  981  Loss  1960.1892031630396\n",
            "Epoch  982  Loss  1952.875588525073\n",
            "Epoch  983  Loss  1955.0929172462186\n",
            "Epoch  984  Loss  1951.476803572293\n",
            "Epoch  985  Loss  1949.2052731968076\n",
            "Epoch  986  Loss  1944.1839476089071\n",
            "Epoch  987  Loss  1943.5972703859966\n",
            "Epoch  988  Loss  1949.0831468652254\n",
            "Epoch  989  Loss  1948.4383678157435\n",
            "Epoch  990  Loss  1941.8904484742195\n",
            "Epoch  991  Loss  1937.2697851022324\n",
            "Epoch  992  Loss  1934.8124667257703\n",
            "Epoch  993  Loss  1932.7723864490079\n",
            "Epoch  994  Loss  1930.365111320653\n",
            "Epoch  995  Loss  1937.0198230949284\n",
            "Epoch  996  Loss  1925.5262157884774\n",
            "Epoch  997  Loss  1923.603733118513\n",
            "Epoch  998  Loss  1922.9052625464064\n",
            "Epoch  999  Loss  1920.4125030768798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj29u2Rwp7u0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3ba35be9-2bb4-43d2-8560-f2742c99ccc5"
      },
      "source": [
        "plt.plot(range(epochs_bayesiano), losses_total_bayesiano)\n",
        "plt.title(\"Epoch vs Loss in bayesian NN\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "McMApU6hUSbn"
      },
      "source": [
        "At the moment of predicting, the neural network does not take the trained model and see the result, but must sample models according to the distributions of its parameters and for each of the models sampled, pass the data seeing its result. Then, compare all the results seeing which value has the highest probability for all models.\n",
        "\n",
        "For these networks, there are two forms of prediction. In the first one the classifier is forced to choose, so that although you are not very sure, you choose the one that you think is best. And the second is that if he has a lot of doubt he refrains from choosing and says that he is not capable of deciding correctly, this is achieved since the network does not deliver an absolute result (\"the student will take this branch\"), but gives a probability that the value is one given the vector delivered.\n",
        "\n",
        "For the first type of prediction, we obtain an approximate value between 40% and 50% (varies)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b2JPDvh3uSRl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ad5166f9-ff42-4af8-f30b-f226f8431de2"
      },
      "source": [
        "num_samples = 100\n",
        "def predict(x):\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(x).cpu().data for model in sampled_models]\n",
        "    mean = torch.mean(torch.stack(yhats), 0)\n",
        "    return mean\n",
        "\n",
        "print('Prediction when network is forced to predict')\n",
        "result = []\n",
        "size = []\n",
        "for data0, data1 in zip(inputs_test_2, outputs_test_2):\n",
        "    predicted = (Multinomial(logits=predict(data0)).mean > 0.01).to(torch.float)\n",
        "    compa = torch.eq(predicted, data1.cpu())\n",
        "    result.append(torch.sum(torch.sum(compa, dim = 1) == 40).item())\n",
        "    size.append(len(output))\n",
        "print(result)\n",
        "print(size)\n",
        "print(\"Accuracy: \", sum(result)/sum(size))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction when network is forced to predict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[66, 67, 63, 68, 65, 65, 55, 60, 61, 66, 62, 57, 59, 56, 64, 63, 62, 66, 63, 65, 55, 68, 69, 55, 59]\n",
            "[144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144]\n",
            "Accuracy:  0.4330555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ZdqJ5o4Vugg"
      },
      "source": [
        "For the second value, if we establish that it has a security greater than 20%, the model refrains from choosing around 650 data and correctly predicts approximately 70%. Also, if we establish that it has a security greater than 30%, the model refrains from choosing around 1300 data and correctly predicts approximately 78% (it can also vary). Then, there is a trade off between how much data the program correctly predicts and how much data it decides to predict, as we will see with the following examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YPc55MDg4Syc",
        "colab": {}
      },
      "source": [
        "num_samples = 100\n",
        "def predict2(x):\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(x).cpu().data for model in sampled_models]\n",
        "    data = [Multinomial(logits = i).mean for i in yhats]\n",
        "    return data\n",
        "\n",
        "def test_batch(data0, data1, percent):\n",
        "    y = predict2(data0)\n",
        "    data = [[[] for _ in range(len(data1[0]))] for _ in range(len(data1))]\n",
        "    \n",
        "    correcto = 0\n",
        "    aceptado = 0\n",
        "    saltado = 0\n",
        "    \n",
        "    for i in range(len(data1)):\n",
        "        for j in range(len(data1[0])):\n",
        "            for k in range(len(y)):\n",
        "                data[i][j].append(y[k][i][j].item())\n",
        "            data[i][j].sort()\n",
        "        prob = (np.percentile(data[i], 50, axis = 1) > percent)\n",
        "        if np.sum(prob) == 0:\n",
        "            saltado += 1\n",
        "        \n",
        "        elif(np.sum(prob == (data1[i].cpu().numpy()==1)) == 40):\n",
        "            correcto += 1\n",
        "            aceptado += 1\n",
        "        else:\n",
        "            aceptado += 1\n",
        "    torch.cuda.empty_cache()\n",
        "    return aceptado, correcto, saltado"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_gvxwspA4ekl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "101e5e4e-cf02-452c-ac53-39525fa5263c"
      },
      "source": [
        "# Prediction when network can decide not to predict\n",
        "print('Prediction when network can refuse')\n",
        "skipped = 0\n",
        "correct = 0\n",
        "accepted = 0\n",
        "for data0, data1 in zip(inputs_test_2, outputs_test_2):\n",
        "    aceptado, correcto, saltado = test_batch(data0, data1, 0.4)\n",
        "    skipped += saltado\n",
        "    correct += correcto\n",
        "    accepted += aceptado\n",
        "\n",
        "print(\"Total data: \", accepted + skipped)\n",
        "print(\"Skipped:\", skipped)\n",
        "print(\"Accuracy when made predictions: %d %%\" % (100 * correct / accepted))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction when network can refuse\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total data:  3606\n",
            "Skipped: 2584\n",
            "Accuracy when made predictions: 95 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zflkNN1zZpSN"
      },
      "source": [
        "## Testing with real values\n",
        "\n",
        "\n",
        "Below are going to take real inputs, not seen by the network, and compare the given values. We are going to look mainly at the courses that the UC engineering program says, and the courses that we (Martin Anselmo and Guillermo Espinosa) have taken as students, since it often happens that the real plan is not followed.\n",
        "\n",
        "![](https://i.imgur.com/F95OXFz.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cjeT_Y6RehOd"
      },
      "source": [
        "We define some functions to construct inputs and test easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KTuGKcffedEi",
        "colab": {}
      },
      "source": [
        "def to_verbose(input_):\n",
        "    # Transform input received by the model to information dictionary\n",
        "    informacion = {\"taken_courses\": [], 'semester_actual': 0, \"major\": \"NA\", \"minor\": \"NA\", \"program\": \"NA\"}\n",
        "\n",
        "    part_courses = input_[:n_semesters*n_courses]\n",
        "    taken = set()\n",
        "    for i in range(n_semesters):\n",
        "#         print(part_courses[i*n_courses: (i+1)*n_courses])\n",
        "        a = np.where(part_courses[i*n_courses:(i+1)*n_courses] == 1)[0]\n",
        "        if len(a):\n",
        "            for k in a:\n",
        "                taken.add(k)\n",
        "            \n",
        "    informacion['taken_courses'] = list(taken)\n",
        "    \n",
        "    \n",
        "    input_ = input_[n_semesters*n_courses:]\n",
        "    informacion['semester_actual'] = np.where(input_[:n_semesters] == 1)[0][0]\n",
        "    input_ = input_[n_semesters:]\n",
        "\n",
        "    informacion['major'] = np.where(input_[:n_majors] == 1)[0][0]\n",
        "    input_ = input_[n_majors:]\n",
        "\n",
        "\n",
        "    informacion['minor'] = np.where(input_[:n_minors] == 1)[0][0]\n",
        "    input_ = input_[n_minors:]\n",
        "\n",
        "    informacion['programa'] = np.where(input_[:n_programs] == 1)[0][0]\n",
        "    input_ = input_[n_programs:]\n",
        "\n",
        "\n",
        "    return informacion\n",
        "  \n",
        "def to_input(informacion):\n",
        "    \"\"\"\n",
        "    Method that given a dictionary, returns the input received by the model\n",
        "    \"\"\"\n",
        "    # information = {\"taken_courses\": {n_semester: [list_courses]}, 'semester_actual': 0, \"major\": \"NA\", \"minor\": \"NA\", \"program\": \"NA\"}\n",
        "    in_ = [0 for i in range(n_semesters*n_courses)] # Semestres_anteriores\n",
        "    in_ += [0 for i in range(n_semesters)] # actual semester\n",
        "    in_ += [0 for i in range(n_majors)]\n",
        "    in_ += [0 for i in range(n_minors)]\n",
        "    in_ += [0 for i in range(n_programs)]\n",
        "\n",
        "\n",
        "            \n",
        "    for n_semester, list_courses in informacion['taken_courses'].items():\n",
        "        for course in list_courses:      \n",
        "              in_[n_courses*n_semester + course] = 1\n",
        "\n",
        "\n",
        "    in_[n_courses*n_semesters + informacion['semester_actual']] = 1\n",
        "\n",
        "    in_[n_courses*n_semesters + n_semesters + informacion['major']] = 1\n",
        "\n",
        "    in_[n_courses*n_semesters + n_semesters + n_majors + informacion['minor']] = 1\n",
        "\n",
        "    in_[n_courses*n_semesters + n_semesters + n_majors + n_minors + informacion['program']] = 1\n",
        "\n",
        "    return in_\n",
        "\n",
        "  \n",
        "def proba_dict(net, X):\n",
        "    \"\"\"\n",
        "    Return a dictionary {0: 0.25, 1: 0.22 ...} with the probabilities of the following courses.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    X= X.to(device)\n",
        "\n",
        "\n",
        "    output = net(X)\n",
        "    letras = [mapping_name[i] for i in range(n_courses)]\n",
        "    probas = [i.item() for i in output[0]]\n",
        "\n",
        "    dict_proba = dict(zip(letras, probas))\n",
        "    return dict_proba\n",
        "  \n",
        "    \n",
        "num_samples = 100\n",
        "def predecir(x):\n",
        "    \"\"\"\n",
        "    Method to predict using the Bayesian network\n",
        "    Returns a list of trues or false, under the percent confidence interval\n",
        "    \"\"\"\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(x.to(device).float()).cpu().data for model in sampled_models]\n",
        "    data = [Multinomial(logits = i).mean.numpy() for i in yhats]\n",
        "    data = np.array(data).transpose()\n",
        "    data = np.sort(data, axis = 1)\n",
        "    prob = np.percentile(data, 50, axis = 1)\n",
        "    del sampled_models\n",
        "    del yhats\n",
        "    del data\n",
        "    torch.cuda.empty_cache()\n",
        "    return prob\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwiSs7aMZ3yu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9912010-60a6-4270-f81e-082a8c8fe252"
      },
      "source": [
        "print('We can see the keys of each course for easier handling and testing')\n",
        "for i, course in enumerate(mapping_name):\n",
        "    print(i, course)\n",
        "    \n",
        "for i, course in enumerate(mapping_major):\n",
        "    print(i, course)\n",
        "\n",
        "for i, course in enumerate(mapping_minor):\n",
        "    print(i, course)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We can see the keys of each course for easier handling and testing\n",
            "0 Matematicas Discretas\n",
            "1 Programacion Avanzada\n",
            "2 Sistemas de Informacion\n",
            "3 Ingenieria de Software\n",
            "4 Bases de Datos\n",
            "5 Visualizacion de Informacion\n",
            "6 Estructuras de Datos y Algorit\n",
            "7 Arquitectura de Sistemas de So\n",
            "8 Sistemas Operativos y Redes\n",
            "9 Mineria de Datos\n",
            "10 Arquitectura de Computadores\n",
            "11 Tecnologias y Aplicaciones Web\n",
            "12 Inteligencia Artificial\n",
            "13 Introduccion a la Programacion\n",
            "14 Proyecto de Especialidad\n",
            "15 Taller de Aplicaciones en Plat\n",
            "16 Sistemas Recomendadores\n",
            "17 Introduccion a  las Aplicacion\n",
            "18 Programacion Como Herramienta\n",
            "19 Computacion: Ciencia y Tecnolo\n",
            "20 Gestion de Proyectos de Tecnol\n",
            "21 Estrategias y Tecnologias de I\n",
            "22 Modelos de Procesos\n",
            "23 Conocimiento, Cultura y Tecnol\n",
            "24 Tecnologias para Gestion Estra\n",
            "25 Sistemas Distribuidos\n",
            "26 Taller de Programacion II\n",
            "27 Testing\n",
            "28 Programacion Concurrente\n",
            "29 Diseno Detallado de Software\n",
            "30 Diseno y Analisis de Algoritmo\n",
            "31 Creacion de Videojuegos\n",
            "32 Procesamiento Avanzado de Imag\n",
            "33 Seminario de Postgrado\n",
            "34 Fundamentos de Procesamiento d\n",
            "35 Diseno Avanzado de Aplicacione\n",
            "36 Teoria de Automatas y Lenguaje\n",
            "37 Investigacion o Proyecto\n",
            "38 Topicos en Ciencia de la Compu\n",
            "39 Laboratorio de Resonancia Magn\n",
            "0 MAJOR EN COMPUTACI√ìN E INGENIER√çA DE SOFTWARE - TRACK INGENIER√çA DE SOFTWARE\n",
            "1 MAJOR EN COMPUTACI√ìN E INGENIER√çA DE SOFTWARE - TRACK COMPUTACI√ìN\n",
            "2 MAJOR EN INVESTIGACI√ìN OPERATIVA\n",
            "3 NA\n",
            "4 MAJOR EN INGENIER√çA, DISE√ëO E INNOVACI√ìN - TRACK EN INGENIER√çA\n",
            "5 MAJOR EN INGENIER√çA MINERA\n",
            "6 MAJOR EN SISTEMAS AUT√ìNOMOS Y ROB√ìTICOS\n",
            "7 MAJOR EN INGENIER√çA EL√âCTRICA\n",
            "8 MAJOR EN COMPUTACI√ìN E INGENIER√çA DE SOFTWARE\n",
            "9 MAJOR EN INGENIER√çA BIOL√ìGICA\n",
            "10 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA Y GESTI√ìN DE LA CONSTRUCCI√ìN\n",
            "11 MAJOR EN INGENIER√çA, DISE√ëO E INNOVACI√ìN - TRACK INGENIER√çA Y DISE√ëO\n",
            "12 MAJOR EN INGENIER√çA QU√çMICA\n",
            "13 MAJOR EN CIENCIAS AMBIENTALES\n",
            "14 MAJOR EN COMPUTACI√ìN E INGENIER√çA DE SOFTWARE - TRACK TECNOLOG√çAS DE LA INFORMACI√ìN\n",
            "15 MAJOR EN SISTEMAS DE TRANSPORTE - √ÅREA 1: EXTERNALIDADES E IMPACTO AMBIENTAL\n",
            "16 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA DE MINER√çA\n",
            "17 MAJOR EN INGENIER√çA BIOM√âDICA - TRACK GENERAL\n",
            "18 MAJOR EN INGENIER√çA HIDR√ÅULICA-TRACK EN INGENIER√çA HIDR√ÅULICA\n",
            "19 MAJOR EN INGENIER√çA DE CONSTRUCCI√ìN\n",
            "20 MAJOR EN INGENIER√çA Y ARQUITECTURA\n",
            "21 MAJOR EN SISTEMAS DE TRANSPORTE - √ÅREA 4: PROFUNDIZACI√ìN EN INGENIER√çA DE TRANSPORTE\n",
            "22 MAJOR EN INGENIER√çA MEC√ÅNICA\n",
            "23 MAJOR EN SISTEMAS DE TRANSPORTE - √ÅREA 2: HERRAMIENTAS DE AN√ÅLISIS Y MODELACI√ìN\n",
            "24 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA AMBIENTAL\n",
            "25 MAJOR EN INGENIER√çA AMBIENTAL\n",
            "26 MAJOR EN GEOCIENCIAS\n",
            "27 MAJOR EN INGENIER√çA MATEM√ÅTICA-TRACK 1: MODELAMIENTO DETERMIN√çSTICO\n",
            "28 MAJOR EN SISTEMAS DE TRANSPORTE - √ÅREA 3: MANEJO DE BASES DE DATOS\n",
            "29 MAJOR EN INGENIER√çA MATEM√ÅTICA - TRACK 1: FUNDAMENTOS DE OPTIMIZACI√ìN\n",
            "30 MAJOR EN INGENIER√çA ESTRUCTURAL\n",
            "31 MAJOR EN INGENIER√çA MATEM√ÅTICA - TRACK 4: TEOR√çA DE LA COMPUTACI√ìN\n",
            "32 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA DE DISE√ëO Y CONSTRUCCI√ìN DE OBRAS\n",
            "33 MAJOR EN INGENIER√çA MATEM√ÅTICA-TRACK 2: MODELAMIENTO ESTOC√ÅSTICO\n",
            "34 MAJOR EN INGENIER√çA MATEM√ÅTICA - TRACK 5: DATA SCIENCE\n",
            "35 MAJOR EN INGENIER√çA MATEM√ÅTICA-TRACK 3: TEOR√çA DE LA COMPUTACI√ìN\n",
            "36 MAJOR EN INGENIER√çA HIDR√ÅULICA - TRACK EN INGENIER√çA HIDR√ÅULICA\n",
            "37 MINOR EN RECURSOS HUMANOS\n",
            "38 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA ESTRUCTURAL\n",
            "39 MAJOR EN INGENIER√çA GEOT√âCNICA\n",
            "40 MAJOR EN INGENIER√çA HIDR√ÅULICA - TRACK EN RECURSOS H√çDRICOS\n",
            "41 MAJOR EN INGENIER√çA HIDR√ÅULICA-TRACK EN RECURSOS H√çDRICOS\n",
            "42 MAJOR EN INGENIER√çA MATEM√ÅTICA - TRACK 3: CUANTIFICACI√ìN DE INCERTIDUMBRE\n",
            "43 MAJOR EN INGENIER√çA MATEM√ÅTICA - TRACK 2: FUNDAMENTOS DE AN√ÅLISIS NUM√âRICO\n",
            "44 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA DE TRANSPORTE\n",
            "45 MAJOR EN INGENIER√çA, DISE√ëO E INNOVACI√ìN - TRACK DISE√ëO\n",
            "46 MAJOR EN INGENIER√çA F√çSICA - TRACK INGENIER√çA\n",
            "47 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA GEOT√âCNICA\n",
            "48 MAJOR EN INGENIER√çA F√çSICA - TRACK F√çSICA\n",
            "49 MAJOR EN INGENIER√çA BIOM√âDICA - TRACK PREMEDICINA\n",
            "50 MAJOR EN INGENIER√çA CIVIL - TRACK INGENIER√çA HIDR√ÅULICA\n",
            "0 MINOR DE PROFUNDIDAD EN DATA SCIENCE Y ANALYTICS\n",
            "1 NA\n",
            "2 MINOR DE AMPLITUD EN MATEM√ÅTICAS APLICADAS\n",
            "3 MINOR DE AMPLITUD EN INGENIER√çA MATEM√ÅTICA\n",
            "4 MINOR DE AMPLITUD EN SISTEMAS DE TRANSPORTE\n",
            "5 MINOR DE AMPLITUD EN INGENIER√çA INDUSTRIAL\n",
            "6 MINOR DE AMPLITUD EN TECNOLOG√çA DE INFORMACI√ìN\n",
            "7 MINOR DE PROFUNDIDAD EN PROCESOS MINEROS\n",
            "8 MINOR DE PROFUNDIDAD EN FUNDAMENTOS CIENT√çFICOS Y TECNOL√ìGICOS DE LA COMPUTACI√ìN\n",
            "9 MINOR DE PROFUNDIDAD EN AUTOMATIZACI√ìN E INTELIGENCIA COMPUTACIONAL - √ÅREA 1: CONTROL Y AUTOMATIZACI√ìN\n",
            "10 MINOR DE PROFUNDIDAD EN ENERG√çA EL√âCTRICA\n",
            "11 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - AREA BIOMEDICINA\n",
            "12 MINOR DE AMPLITUD EN HIDROLOG√çA AMBIENTAL\n",
            "13 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - √ÅREA PROCESOS\n",
            "14 MINOR DE AMPLITUD EN INGENIER√çA MEC√ÅNICA\n",
            "15 MINOR DE PROFUNDIDAD EN HIDROGEOQU√çMICA\n",
            "16 MINOR DE PROFUNDIDAD EN AUTOMATIZACI√ìN E INTELIGENCIA COMPUTACIONAL - √ÅREA 2: INTELIGENCIA COMPUTACIONAL\n",
            "17 MINOR DE AMPLITUD EN AGUA Y MINER√çA\n",
            "18 MINOR DE AMPLITUD EN EXTERNALIDADES DE TRANSPORTE\n",
            "19 MINOR DE AMPLITUD EN PROGRAMACI√ìN\n",
            "20 MINOR DE PROFUNDIDAD EN ELECTR√ìNICA Y TELECOMUNICACIONES\n",
            "21 MINOR DE PROFUNDIDAD EN INGENIER√çA BIOM√âDICA\n",
            "22 MINOR DE PROFUNDIDAD EN HIDROGEOLOG√çA\n",
            "23 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA DE CONSTRUCCI√ìN\n",
            "24 MINOR DE PROFUNDIDAD INGENIER√çA Y ARQUITECTURA\n",
            "25 MINOR DE AMPLITUD EN FUNDAMENTOS PARA LA GESTI√ìN MINERA\n",
            "26 MINOR DE AMPLITUD EN INGENIER√çA DE CONSTRUCCI√ìN\n",
            "27 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA DE TRANSPORTE - √ÅREA 2: HERRAMIENTAS DE AN√ÅLISIS Y MODELACI√ìN\n",
            "28 MINOR DE PROFUNDIDAD EN MATERIALES\n",
            "29 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA DE TRANSPORTE - √ÅREA 4: PROFUNDIZACI√ìN EN INGENIER√çA DE TRANSPORTE\n",
            "30 MINOR DE AMPLITUD EN LOG√çSTICA Y TRANSPORTE DE CARGA\n",
            "31 MINOR DE PROFUNDIDAD EN MECATR√ìNICA\n",
            "32 MINOR DE AMPLITUD EN AGUA Y ENERG√çA\n",
            "33 MINOR DE PROFUNDIDAD EN PELIGROS GEOL√ìGICOS\n",
            "34 MINOR DE AMPLITUD EN INGENIER√çA QU√çMICA\n",
            "35 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA-TRACK 1: MODELAMIENTO DETERMIN√çSTICO\n",
            "36 MINOR DE AMPLITUD EN INGENIER√çA EL√âCTRICA\n",
            "37 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA DE TRANSPORTE - √ÅREA 1: EXTERNALIDADES E IMPACTO AMBIENTAL\n",
            "38 MINOR DE PROFUNDIDAD EN AUTOMATIZACI√ìN E INTELIGENCIA COMPUTACIONAL-√ÅREA 1: CONTROL Y AUTOMATIZACI√ìN\n",
            "39 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA CIVIL\n",
            "40 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA - TRACK 1: FUNDAMENTOS DE OPTIMIZACI√ìN\n",
            "41 MINOR DE PROFUNDIDAD EN IM√ÅGENES M√âDICAS\n",
            "42 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA-TRACK 2: MODELAMIENTO ESTOC√ÅSTICO\n",
            "43 MINOR DE AMPLITUD EN FUNDAMENTOS DE INGENIER√çA AEROESPACIAL\n",
            "44 MINOR DE PROFUNDIDAD EN INGENIER√çA DE PROCESOS\n",
            "45 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - √ÅREA MEDIOAMBIENTE\n",
            "46 MINOR DE PROFUNDIDAD EN GESTI√ìN MINERA\n",
            "47 MINOR DE AMPLITUD EN INGENIER√çA GEOT√âCNICA\n",
            "48 MINOR EN RECURSOS HUMANOS\n",
            "49 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA-√ÅREA BIOMATERIALES\n",
            "50 MINOR DE AMPLITUD EN FUNDAMENTOS DE INGENIER√çA BIOL√ìGICA\n",
            "51 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA - TRACK 2: FUNDAMENTOS DE AN√ÅLISIS NUM√âRICO\n",
            "52 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA - TRACK 4: TEOR√çA DE LA COMPUTACI√ìN\n",
            "53 MINOR DE AMPLITUD EN GEOCIENCIAS - √ÅREA PELIGROS GEOL√ìGICOS\n",
            "54 MINOR DE PROFUNDIDAD EN GEOLOG√çA AMBIENTAL\n",
            "55 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA-√ÅREA PROCESOS\n",
            "56 MINOR DE PROFUNDIDAD EN AUTOM√ÅTICA Y ROB√ìTICA\n",
            "57 MINOR DE AMPLITUD EN FUNDAMENTOS DE PROCESOS MINEROS\n",
            "58 MINOR DE PROFUNDIDAD EN BIOMATERIALES\n",
            "59 MINOR DE AMPLITUD EN INGENIER√çA ESTRUCTURAL\n",
            "60 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA - TRACK 3: CUANTIFICACI√ìN DE INCERTIDUMBRE\n",
            "61 MINOR DE AMPLITUD EN OBRAS HIDR√ÅULICAS\n",
            "62 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - √ÅREA BIOLOG√çA SINT√âTICA\n",
            "63 MINOR DE PROFUNDIDAD EN TECNOLOG√çA AMBIENTAL\n",
            "64 MINOR DE PROFUNDIDAD ARTICULACI√ìN PROYECTOS DE DISE√ëO\n",
            "65 MINOR DE AMPLITUD EN SISTEMA DE TRATAMIENTO DE AGUA\n",
            "66 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - √ÅREA BIOINFORM√ÅTICA\n",
            "67 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - AREA MEDIOAMBIENTE\n",
            "68 MINOR DE PROFUNDIDAD EN INGENIER√çA DE ALIMENTOS\n",
            "69 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - √ÅREA BIOMEDICINA\n",
            "70 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - AREA BIOINFORM√ÅTICA\n",
            "71 MINOR DE AMPLITUD EN GEOCIENCIAS - √ÅREA RECURSOS GEOL√ìGICOS\n",
            "72 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA CIVIL (MAJOR EN INGENIER√çA Y ARQUITECTURA)\n",
            "73 MINOR DE PROFUNDIDAD EN TEOR√çA Y APLICACI√ìN DE INGENIER√çA MATEM√ÅTICA - TRACK 5: DATA SCIENCE\n",
            "74 MINOR DE PROFUNDIDAD EN BIOMEC√ÅNICA\n",
            "75 MINOR DE PROFUNDIDAD ARTICULACI√ìN PREMEDICINA\n",
            "76 MINOR DE PROFUNDIDAD ARTICULACI√ìN INGENIER√çA DE TRANSPORTE - √ÅREA 3: MANEJO DE BASES DE DATOS\n",
            "77 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - AREA BIOLOG√çA SINT√âTICA\n",
            "78 MINOR DE PROFUNDIDAD EN BIOINGENIER√çA - AREA PROCESOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "25HfgeMVZsxs"
      },
      "source": [
        "### Red no bayesiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZIiKBdMfZwpq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "3078f334-ca69-43e9-9bd0-e179ec30d0b4"
      },
      "source": [
        "# This is how the information should look\n",
        "# information = {\"taken_course\": {n_semester: [list_course]}, 'semester_actual': 0, \"major\": \"NA\", \"minor\": \"NA\", \"program\": \"NA\"}\n",
        "\n",
        "print(f\"We are going to define semester by semester, in order to compare real results with those obtained\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "second_semester = {\"taken_courses\": {0: []}, \"semester_actual\": 1, \"major\": 3, \"minor\": 1, \"program\": 0}\n",
        "in_ = to_input(second_semester)\n",
        "\n",
        "print(\"How information is seen in a simplified way\")\n",
        "print(second_semester)\n",
        "print(\"What does the information that the network receives as input look like?\")\n",
        "print(in_)\n",
        "\n",
        "probs = proba_dict(net, torch.tensor(in_, dtype=torch.float).view(-1, 1443))\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "print(\"If we stay with the predictions where the value obtained is positive, we have the following\")\n",
        "\n",
        "for k in sorted(probs, key=probs.get, reverse=True):\n",
        "    if probs[k] >= 0:\n",
        "      print(k, probs[k])\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are going to define semester by semester, in order to compare real results with those obtained\n",
            "\n",
            "\n",
            "\n",
            "How information is seen in a simplified way\n",
            "{'taken_courses': {0: []}, 'semester_actual': 1, 'major': 3, 'minor': 1, 'program': 0}\n",
            "What does the information that the network receives as input look like?\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "If we stay with the predictions where the value obtained is positive, we have the following\n",
            "Introduccion a la Programacion 5.462839126586914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u0oJnly9fCXW"
      },
      "source": [
        "As you can see, the second semester achieves perfect acores, predicting only introduction to programming, let's see more semesters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yu2MCGOzg47u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8c835b0-c054-4108-ec12-c5e7136a8a57"
      },
      "source": [
        "third_semester = {\"taken_courses\": {1: [13]}, \"semester_actual\": 9, \"major\": 0, \"minor\": 0, \"program\": 0}\n",
        "in_ = to_input(third_semester)\n",
        "probs = proba_dict(net, torch.tensor(in_, dtype=torch.float).view(-1, 1443))\n",
        "\n",
        "for k in sorted(probs, key=probs.get, reverse=True):\n",
        "    if probs[k] >= 0:\n",
        "      print(k, probs[k])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Programacion Avanzada 0.45078128576278687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3AFUzlNZhl1S"
      },
      "source": [
        "We can see how the problems mentioned above begin to emerge, the network is overfitted with introduction to programming, which is the course with more data. Another problem that can easily be noticed is that the course that would correspond would be the exploratory of the major in computing, and despite the fact that it is in the first 5, a student who has made an introduction to programming will not necessarily take it, since that there are many majors apart from computing, where these courses are not followed. This is an inherent problem in conventional networks, since it is forced to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e87ipYZtiHl1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "58ca8db4-af36-40a2-9ccb-d7985ebcdf9d"
      },
      "source": [
        "sixth_semester = {\"taken_courses\": {1: [13], 2: [19], 3: [1], 4: [3, 0], 5: [6, 29, 4]}, \"semester_actual\": 9, \"major\": 0, \"minor\": 0, \"program\": 0}\n",
        "in_ = to_input(sixth_semester)\n",
        "probs = proba_dict(net, torch.tensor(in_, dtype=torch.float).view(-1, 1443))\n",
        "\n",
        "for k in sorted(probs, key=probs.get, reverse=True):\n",
        "    if probs[k] >= 0:\n",
        "      print(k, probs[k])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Diseno Detallado de Software 2.0320682525634766\n",
            "Tecnologias y Aplicaciones Web 0.6052731275558472\n",
            "Visualizacion de Informacion 0.4547760486602783\n",
            "Arquitectura de Sistemas de So 0.34479832649230957\n",
            "Inteligencia Artificial 0.1360008716583252\n",
            "Sistemas de Informacion 0.014374583959579468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ndncm9GcjHV-"
      },
      "source": [
        "Here you can see a good progress, but the network chooses to predict many more courses than it should, and even a repeated one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_6zutGW0ZyqX"
      },
      "source": [
        "### Red bayesiana"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ov_2JgvQtfJE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3149ab06-bc99-43ae-e0bd-92887e081015"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "second_semester = {\"taken_courses\": {0: []}, \"semester_actual\": 1, \"major\": 3, \"minor\": 1, \"program\": 0}\n",
        "in_ = to_input(second_semester)\n",
        "mapping_name[predecir(torch.tensor(in_)) > 0.1]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Introduccion a la Programacion'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xp9vGQW7t3KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "44314731-93de-4a8f-b387-2ecaad022745"
      },
      "source": [
        "third_semester = {\"taken_courses\": {1: [13]}, \"semester_actual\": 9, \"major\": 0, \"minor\": 0, \"program\": 0}\n",
        "in_ = to_input(third_semester)\n",
        "mapping_name[predecir(torch.tensor(in_)) > 0.1]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hBSOZfGxYc_p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6f3f04be-efe6-4085-88c4-dba5373a62b6"
      },
      "source": [
        "sixth_semester = {\"taken_courses\": {1: [13], 2: [19], 3: [1], 4: [3, 0], 5: [6, 29, 4]}, \"semester_actual\": 9, \"major\": 0, \"minor\": 0, \"program\": 0}\n",
        "in_ = to_input(sixth_semester)\n",
        "mapping_name[predecir(torch.tensor(in_)) > 0.1]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ktLfXlTmDeUx"
      },
      "source": [
        "\n",
        "## Results and analysis\n",
        "\n",
        "The Bayesian network developed allows us to estimate which courses a student will take the following semester with an estimated degree of certainty. Although the results are not yet conclusive regarding the reliability of the recommender, they are heading towards what is a good vacancie recommender system, in which it conveniently estimates that each student will take and gives a degree of reliability to that choice. This recommender is expected to be upgradeable through the complexity of the network structure, allowing it to do not only a linear or quadratic analysis of the data (which is what usually happens with a layer), but also take advantage of convolutional capabilities and a deep learning for better analysis.\n",
        "\n",
        "Even so, for the data estimated with confidence, good results were achieved, therefore the classifier manages to have an intuition about courses that are taken by the students even with the simplicity of the structure. Furthermore, choosing courses can be considered a difficult problem, from the point of view that a student can choose to postpone or take a courses for various reasons, in which two students who have taken the same courses can take different things next semester.\n",
        "\n",
        "One problem in how to calculate classifier performance is that it only says that it is right at the moment that one correctly predicts all the branches that a student will take the next semester. Perhaps another metric that allows us to see which fields we are successful in and which does not allow us to see a better performance in the data given the great demand that the metric has."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KTeQjppoWf-r"
      },
      "source": [
        "## Conclusions and projections\n",
        "\n",
        "From a probabilistic model that establishes a non-fixed model for the network, we can create a selection system that has better performance and is able to say that it does not know when it is not certain. In this way it is possible to classify better and for the rest not classified, analyze them in another way. Since the choice of branches is different for each student, a classifier that looks linearly or squarely at the data and sees direct correlations is not able to correctly classify the choice of students. Still, we can see that it is an effective method, that it is able to intuit the branches that a student is going to take and manages to make good predictions when he is confident in what he says.\n",
        "\n",
        "## Bibliography\n",
        "\n",
        "https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0pc-tvBf9ae",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}